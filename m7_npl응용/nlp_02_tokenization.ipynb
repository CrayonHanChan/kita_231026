{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/ratsgo/nlpbook/blob/master/examples/basic/tokenization.ipynb","timestamp":1672842934226}],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"5c579f2e85034b54958eebe0aef48e16":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_de6194770cf3442eaf08ded83794ddc8","IPY_MODEL_f6fa16bc82aa411995b0b37d4e6ea89d","IPY_MODEL_9dc47c629b5249bcb3b434b820f79289"],"layout":"IPY_MODEL_ed082eccba74464391f33122f2d08fa2"}},"de6194770cf3442eaf08ded83794ddc8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0d3c00069a74e438798e563d57494f8","placeholder":"​","style":"IPY_MODEL_5bb46b1fd3944cab917aa6807f714b23","value":"vocab.txt: 100%"}},"f6fa16bc82aa411995b0b37d4e6ea89d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7239f427ffe04aeba97e52ec7a214e44","max":77779,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1c85fa68214d473c8f76cffcaf299fd8","value":77779}},"9dc47c629b5249bcb3b434b820f79289":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b4733361c8042c3836962a0eb7c5fce","placeholder":"​","style":"IPY_MODEL_b9cac00b8fea4cfc8b6b2d94c7117862","value":" 77.8k/77.8k [00:00&lt;00:00, 3.94MB/s]"}},"ed082eccba74464391f33122f2d08fa2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0d3c00069a74e438798e563d57494f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5bb46b1fd3944cab917aa6807f714b23":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7239f427ffe04aeba97e52ec7a214e44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c85fa68214d473c8f76cffcaf299fd8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8b4733361c8042c3836962a0eb7c5fce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9cac00b8fea4cfc8b6b2d94c7117862":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d1e4569e2554ec3837e3ad7a3bef347":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_14f763ec82ac43afa2e45f1df9c5c429","IPY_MODEL_6fe1ed43d100432db5cdb72431763ed3","IPY_MODEL_9915fea7665a45a680ddd3b42112b82a"],"layout":"IPY_MODEL_b149025e36fe4605807e3ec0d3296d00"}},"14f763ec82ac43afa2e45f1df9c5c429":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_47206da8b3744a12a948640f12756893","placeholder":"​","style":"IPY_MODEL_07f2ebcaa3f74275b78fcdcd8a1876ba","value":"tokenizer_config.json: 100%"}},"6fe1ed43d100432db5cdb72431763ed3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a311d52ccfe6443ca4372d128a665108","max":51,"min":0,"orientation":"horizontal","style":"IPY_MODEL_63a276a454194391860b2261163408c3","value":51}},"9915fea7665a45a680ddd3b42112b82a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db11c048536b4d8296afcf3249c4a923","placeholder":"​","style":"IPY_MODEL_803b3620f4c74d0f8e6617f7062d50b1","value":" 51.0/51.0 [00:00&lt;00:00, 4.10kB/s]"}},"b149025e36fe4605807e3ec0d3296d00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47206da8b3744a12a948640f12756893":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07f2ebcaa3f74275b78fcdcd8a1876ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a311d52ccfe6443ca4372d128a665108":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63a276a454194391860b2261163408c3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"db11c048536b4d8296afcf3249c4a923":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"803b3620f4c74d0f8e6617f7062d50b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c2175fba71b46189968f0db335c5d9d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_10cf88d20db34799890b96cc7cea0e8d","IPY_MODEL_67b98310133545d7944a586a15ad1dff","IPY_MODEL_ff184e2ac93948899137c27b786e5902"],"layout":"IPY_MODEL_a5caeabc8ba549568cb7a5e3a38ac10c"}},"10cf88d20db34799890b96cc7cea0e8d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0523adc87bd24f44aa12106c0eb9511c","placeholder":"​","style":"IPY_MODEL_57d9aa7b4c154858acc777a55674f2b2","value":"config.json: 100%"}},"67b98310133545d7944a586a15ad1dff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d709eedf9e444a2ba663fe2c2bcb94c","max":426,"min":0,"orientation":"horizontal","style":"IPY_MODEL_403835f603074ff9b5780b817f569a9e","value":426}},"ff184e2ac93948899137c27b786e5902":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f87b5321878e44819d156200d235b2a7","placeholder":"​","style":"IPY_MODEL_05683719d1aa4e8eb0bd420e951f9e57","value":" 426/426 [00:00&lt;00:00, 41.4kB/s]"}},"a5caeabc8ba549568cb7a5e3a38ac10c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0523adc87bd24f44aa12106c0eb9511c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57d9aa7b4c154858acc777a55674f2b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d709eedf9e444a2ba663fe2c2bcb94c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"403835f603074ff9b5780b817f569a9e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f87b5321878e44819d156200d235b2a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05683719d1aa4e8eb0bd420e951f9e57":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c7aee878aefc4ca8bb9bacd3d14f98a6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_42a69ee07eac477db0358d220610a0cb","IPY_MODEL_8ae67b6d46204cc6b37e58fe3585cd5a","IPY_MODEL_8da52c0aa209442b887067ea9f8c644d"],"layout":"IPY_MODEL_ad606911a98a4918bd6cce9b9306e099"}},"42a69ee07eac477db0358d220610a0cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4eeffa709b854b4ea049686c965e5a2a","placeholder":"​","style":"IPY_MODEL_bf139c111a8e42dc84307526ee64b1e5","value":"tokenizer.json: 100%"}},"8ae67b6d46204cc6b37e58fe3585cd5a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6594ed47a26e43a8bc7e262245ef514c","max":2825034,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d92a911759234200b45fac4e288933fc","value":2825034}},"8da52c0aa209442b887067ea9f8c644d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5660a55ddaba46eb9ebeb2e7f4467027","placeholder":"​","style":"IPY_MODEL_c282ae71943b4ff29b5f7dc120f5be24","value":" 2.83M/2.83M [00:00&lt;00:00, 3.45MB/s]"}},"ad606911a98a4918bd6cce9b9306e099":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4eeffa709b854b4ea049686c965e5a2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf139c111a8e42dc84307526ee64b1e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6594ed47a26e43a8bc7e262245ef514c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d92a911759234200b45fac4e288933fc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5660a55ddaba46eb9ebeb2e7f4467027":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c282ae71943b4ff29b5f7dc120f5be24":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","metadata":{"id":"EpjMKSVWs6B2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708474260688,"user_tz":-540,"elapsed":30692,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"b0725e7d-e259-4de9-d9ee-87cddce6c4b5"},"source":["from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"t8TJkXkpDnSq","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1708474416483,"user_tz":-540,"elapsed":19424,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"7438f216-3b50-4708-c375-438469978745"},"source":["# 의존성 패키지 설치하기\n","!pip install ratsnlp"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ratsnlp\n","  Downloading ratsnlp-1.0.53-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytorch-lightning==1.6.1 (from ratsnlp)\n","  Downloading pytorch_lightning-1.6.1-py3-none-any.whl (582 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m582.5/582.5 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers==4.28.1 (from ratsnlp)\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Korpora>=0.2.0 (from ratsnlp)\n","  Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flask>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ratsnlp) (2.2.5)\n","Collecting flask-ngrok>=0.0.25 (from ratsnlp)\n","  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n","Collecting flask-cors>=3.0.10 (from ratsnlp)\n","  Downloading Flask_Cors-4.0.0-py2.py3-none-any.whl (14 kB)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (1.25.2)\n","Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (2.1.0+cu121)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (4.66.2)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (6.0.1)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (2023.6.0)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (2.15.2)\n","Collecting torchmetrics>=0.4.1 (from pytorch-lightning==1.6.1->ratsnlp)\n","  Downloading torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.4/840.4 kB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyDeprecate<0.4.0,>=0.3.1 (from pytorch-lightning==1.6.1->ratsnlp)\n","  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (23.2)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (4.9.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1->ratsnlp) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1->ratsnlp) (0.20.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1->ratsnlp) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1->ratsnlp) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.1->ratsnlp)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask>=1.1.4->ratsnlp) (3.0.1)\n","Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask>=1.1.4->ratsnlp) (3.1.3)\n","Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask>=1.1.4->ratsnlp) (2.1.2)\n","Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask>=1.1.4->ratsnlp) (8.1.7)\n","Collecting dataclasses>=0.6 (from Korpora>=0.2.0->ratsnlp)\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from Korpora>=0.2.0->ratsnlp) (2.0.1)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (3.9.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask>=1.1.4->ratsnlp) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.1->ratsnlp) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.1->ratsnlp) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.1->ratsnlp) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.1->ratsnlp) (2024.2.2)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.60.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.5.2)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.20.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (67.7.2)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.7.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning==1.6.1->ratsnlp) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning==1.6.1->ratsnlp) (3.2.1)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning==1.6.1->ratsnlp) (2.1.0)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics>=0.4.1->pytorch-lightning==1.6.1->ratsnlp)\n","  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (4.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.*->pytorch-lightning==1.6.1->ratsnlp) (1.3.0)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.2.2)\n","Installing collected packages: tokenizers, dataclasses, pyDeprecate, lightning-utilities, Korpora, transformers, torchmetrics, flask-ngrok, flask-cors, pytorch-lightning, ratsnlp\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.15.2\n","    Uninstalling tokenizers-0.15.2:\n","      Successfully uninstalled tokenizers-0.15.2\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.35.2\n","    Uninstalling transformers-4.35.2:\n","      Successfully uninstalled transformers-4.35.2\n","Successfully installed Korpora-0.2.0 dataclasses-0.6 flask-cors-4.0.0 flask-ngrok-0.0.25 lightning-utilities-0.10.1 pyDeprecate-0.3.2 pytorch-lightning-1.6.1 ratsnlp-1.0.53 tokenizers-0.13.3 torchmetrics-1.3.1 transformers-4.28.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["dataclasses"]}}},"metadata":{}}]},{"cell_type":"markdown","source":["### 트랜스포머 기반 모델 입력 데이터를 모델에 제공하기 위해 필요한 주요 구성 요소\n","\n","구성 요소들은 모델이 텍스트 데이터를 이해하고 처리할 수 있도록 정보를 인코딩합니다.\n","\n","1. input_ids\n","텍스트를 토크나이즈한 후, 각 토큰을 해당하는 어휘 사전(vocabulary)의 인덱스로 변환한 것입니다. 즉, 입력 텍스트가 모델이 이해할 수 있는 숫자의 시퀀스로 변환된 형태입니다. 이러한 변환을 통해, 모델은 텍스트 데이터를 처리할 수 있게 됩니다.\n","2. token_type_ids\n","주로 BERT와 같이 입력이 여러 개의 세그먼트로 구성될 수 있는 모델에서 사용됩니다. 예를 들어, 질문-응답 태스크나 텍스트 쌍을 처리하는 태스크에서, 두 개의 다른 텍스트 세그먼트를 구분하기 위해 사용됩니다. 각 토큰이 속한 세그먼트를 나타내는 인덱스로, 대개 0과 1의 값을 가집니다. 이를 통해 모델은 두 개의 세그먼트를 구분하고, 관계를 올바르게 이해할 수 있습니다.\n","  - 질문-응답 태스크란 질문(question)에 답(answer)을 하는 과제입니다. 예를들어 질문에 대한 답을 지문(context)에서 찾는 걸 가리킵니다. 질의응답(Question Answering, QA) 태스크에서는 지문(context)과 질문(question)을 구분하기 위해 token_type_ids를 사용\n","\n","    {\n","    \"context\": \"BERT는 구글이 개발한 사전 훈련된 딥 러닝 자연어 처리 모델입니다. 이 모델은 다양한 자연어 처리 태스크에서 뛰어난 성능을 보여줍니다.\",<br>\n","    \"question\": \"BERT를 개발한 회사는 어디인가요?\",<br>\n","    \"answer\": {\n","      \"text\": \"구글\",\n","      \"start_index\": 12,\n","      \"end_index\": 14\n","    }\n","  }\n","\n","    [ token_type_ids ]\n","    \n","    0 (CLS에 해당), 0 (지문 내용에 해당), 0 ([SEP]에 해당), 1 (질문 내용에 해당), 1 ([SEP]에 해당)\n","\n","  - 텍스트 쌍을 처리하는 태스크의 대표 예시로 자연어 추론(Natural Langugage Inference; NLI)이 있습니다. 두 개 문장이 참(entailment), 거짓(contradiction), 중립(neutral)인지 가려내는 것입니다.\n","3. attention_mask\n","모델이 특정 토큰에 주의(attention)를 기울여야 하는지를 나타내는 이진 마스크입니다. 주로 패딩(padding)된 토큰을 처리할 때 사용됩니다. 트랜스포머 모델은 일반적으로 고정된 길이의 입력을 요구하기 때문에, 짧은 텍스트를 입력할 때는 나머지 부분을 특정 값(예: 0)으로 채워 넣어야 합니다. 이러한 패딩된 부분을 모델이 실제 데이터로 처리하지 않도록 attention_mask가 사용됩니다. 패딩된 위치는 0으로, 실제 데이터가 있는 위치는 1로 설정하여 모델이 패딩 부분을 무시하도록 합니다.\n","\n","이 세 가지 구성 요소는 모델이 입력 데이터를 효과적으로 처리하고, 다양한 NLP 태스크에서 좋은 성능을 발휘할 수 있도록 돕습니다."],"metadata":{"id":"DhvOW35ea45-"}},{"cell_type":"markdown","metadata":{"id":"-Jsq7yrjWYTB"},"source":["### GPT 입력값 만들기 - 기 작성한 bpe 활용\n","\n","GPT 입력값을 만들려면 토크나이저부터 준비해야 하며 아래 코드를 수행하면 GPT 모델이 사용하는 토크나이저를 초기화할 수 있다. 먼저 자신의 구글 드라이브 경로(/gdrive/My Drive/nlpbook/bbpe)에는 이전 실습에서 만든 바이트 기준 BPE 어휘 집합(vocab.json)과 바이그램 쌍의 병합 우선순위(merge.txt)가 있어야 합니다."]},{"cell_type":"code","metadata":{"id":"aJRVVpC-WyIc","executionInfo":{"status":"ok","timestamp":1708475618378,"user_tz":-540,"elapsed":2909,"user":{"displayName":"한정현","userId":"04742589720279403748"}}},"source":["# GPT 토크나이저 선언: GPT2Tokenizer'는 텍스트를 GPT-2 모델이 이해할 수 있는 형식으로 변환\n","# 여기에는 텍스트를 토큰(단어 또는 하위 단어)으로 분할하고 이러한 토큰을 모델이 처리에 사용하는 숫자 ID로 변환하는 작업이 포함\n","from transformers import GPT2Tokenizer # transformers 라이브러리에서 GPT2Tokenizer 클래스를 가져온다.\n","tokenizer_gpt = GPT2Tokenizer.from_pretrained(\"/gdrive/My Drive/nlpbook/bbpe\") # 사전 훈련된 토크나이저 로드\n","tokenizer_gpt.pad_token = \"[PAD]\" # 토크나이저는 패딩 목적으로 사용할 토큰을 알아야 하기 때문에 'pad_token'을 명시적으로 설정하는 것이 필요"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["- 패딩(Padding): 딥러닝 모델, 특히 자연어 처리에서, 모델이 배치 처리를 할 때 모든 입력 시퀀스가 동일한 길이를 가져야 합니다. 그렇지 않을 경우, 텐서 연산이 제대로 수행될 수 없습니다. 입력 시퀀스의 길이를 통일하기 위해 짧은 시퀀스의 끝에 '패딩'을 추가하는데, 이때 사용되는 것이 패딩 토큰.\n","\n","- 패딩 토큰(\"[PAD]\"): 실제 데이터가 아닌, 데이터 길이를 맞추기 위해 추가하는 특별한 토큰입니다. \"[PAD]\"는 이러한 목적을 위해 임의로 지정된 문자열.\n","\n","- tokenizer_gpt.pad_token = \"[PAD]\"라는 코드는 GPT-2 토크나이저에게 입력 데이터를 처리할 때 사용할 패딩 토큰으로 \"[PAD]\"를 사용하라고 지시합니다. 이렇게 설정함으로써, 토크나이저는 입력 시퀀스의 길이가 모델이 요구하는 길이에 미치지 못할 때, \"[PAD]\" 토큰을 추가하여 길이를 맞추게 됨.\n","\n","- 이 설정은 특히 배치 처리를 할 때 중요합니다. 배치 처리는 여러 입력 데이터를 동시에 모델에 전달하여 처리하는 것을 말하는데, 모든 입력 데이터가 같은 길이를 가져야 효율적으로 처리할 수 있습니다."],"metadata":{"id":"21Sd3S5bHIM0"}},{"cell_type":"markdown","source":["- merges.txt 병합우선순위\n","- vocab.json 단어사전"],"metadata":{"id":"_7OVFdMyFeLI"}},{"cell_type":"markdown","metadata":{"id":"8n3JlkUbZobu"},"source":["예시 문장 세 개를 각각 토큰화해보겠습니다."]},{"cell_type":"code","metadata":{"id":"Vp48wVBIZtYj","executionInfo":{"status":"ok","timestamp":1708475907181,"user_tz":-540,"elapsed":1,"user":{"displayName":"한정현","userId":"04742589720279403748"}}},"source":["# 토크나이저로 토큰화하기\n","sentences = [\n","    \"아 더빙.. 진짜 짜증나네요 목소리\",\n","    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n","    \"별루 였다..\",\n","]\n","tokenized_sentences = [tokenizer_gpt.tokenize(sentence) for sentence in sentences]"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8-ahPBg-Zx71"},"source":["토큰화 결과를 확인합니다."]},{"cell_type":"code","metadata":{"id":"__DWz4djZz_u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708475910099,"user_tz":-540,"elapsed":518,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"45b97e30-3ce8-4729-c8fc-5c5939fcd020"},"source":["# GPT 모델은 바이트 기준 BPE를 적용\n","tokenized_sentences"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['ìķĦ', 'ĠëįĶë¹Ļ', '..', 'Ġì§Ħì§ľ', 'Ġì§ľì¦ĿëĤĺ', 'ëĦ¤ìļĶ', 'Ġëª©ìĨĮë¦¬'],\n"," ['íĿł',\n","  '...',\n","  'íı¬ìĬ¤íĦ°',\n","  'ë³´ê³ł',\n","  'Ġì´ĪëĶ©',\n","  'ìĺģíĻĶ',\n","  'ì¤Ħ',\n","  '....',\n","  'ìĺ¤ë²Ħ',\n","  'ìĹ°ê¸°',\n","  'ì¡°ì°¨',\n","  'Ġê°Ģë³į',\n","  'ì§Ģ',\n","  'ĠìķĬ',\n","  'êµ¬ëĤĺ'],\n"," ['ë³Ħë£¨', 'Ġìĺ', 'Ģëĭ¤', '..']]"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["GPT 모델 입력 만들기"],"metadata":{"id":"Ee8tN4sQWOzk"}},{"cell_type":"code","metadata":{"id":"ATwWfngCXQXK","executionInfo":{"status":"ok","timestamp":1708475985733,"user_tz":-540,"elapsed":2,"user":{"displayName":"한정현","userId":"04742589720279403748"}}},"source":["#  GPT 모델 입력\n","sentences = [\n","    \"아 더빙.. 진짜 짜증나네요 목소리\",\n","    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n","    \"별루 였다..\",\n","]\n","batch_inputs = tokenizer_gpt(\n","    sentences,\n","    padding=\"max_length\", # 문자의 최대 길이에 맞춰 패딩. max_length 매개변수로 지정된 최대 길이로 균일하게 시퀀스를 채우도록 지시\n","    max_length=12, # 문장의 토큰 기준 최대 길이. 문장이 12개 이상의 토큰으로 변환되면 잘림. 더 적은 수로 변환되면 패딩 토큰으로 채워진다.\n","    truncation=True, # 문장 잘림 허용 옵션. truncation=False로 설정되어 있어 토큰 수가 max_length보다 길 경우, 오류를 발생 가능. 12개가 넘으면 잘림.\n",")"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F9g0tJ9uXj6O"},"source":["`batch_inputs`의 내용을 확인\n","- input_ids: 문장의 토큰화된 표현을 포함하며, 각 토큰은 토크나이저 어휘에서 해당하는 ID로 대체.\n","- attention_mask: 각 위치에 토큰이 존재하는지를 나타내는 이진 마스크입니다. 토큰이 존재하면 1, 토큰이 존재하지 않으면(즉, 패딩인 경우) 0입니다. 이는 모델이 실제 데이터와 패딩을 구별할 수 있도록 도와준다.\n","- GPT는 주어진 문맥을 기반으로 다음에 올 텍스트를 예측하는 구조이기 때문에, token_type_ids와 같은 추가적인 입력 정보가 필요하지 않습니다."]},{"cell_type":"code","metadata":{"id":"T4pnC6DjXkv6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708476429077,"user_tz":-540,"elapsed":499,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"6582aa37-8b1e-480c-92ce-c0c9c19a94e2"},"source":["batch_inputs.keys()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['input_ids', 'attention_mask'])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"XJBRdJegXsZd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708476443363,"user_tz":-540,"elapsed":513,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"7caae495-edf7-40f3-827c-9ae986ffacf1"},"source":["# 'input_ids'는 토큰화 결과를 가지고 각 토큰들을 인덱스(index)로 바꾼 것\n","batch_inputs['input_ids']"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[334, 2338, 263, 581, 4055, 464, 3808, 0, 0, 0, 0, 0],\n"," [3693, 336, 2876, 758, 2883, 356, 806, 422, 9875, 875, 2960, 7292],\n"," [4957, 451, 3653, 263, 0, 0, 0, 0, 0, 0, 0, 0]]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"b_XT5QTsXvBG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708476463195,"user_tz":-540,"elapsed":1051,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"c34eb0a4-2835-4790-fcbd-aea0597ec3c9"},"source":["# attention_mask는 일반 토큰이 자리한 곳(1)과 패딩 토큰이 자리한 곳(0)을 구분해 알려주는 장치\n","batch_inputs['attention_mask']"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n"," [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n"," [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["### 사전 훈련된 KoGPT2 (한국어 GPT-2) Ver 2.0"],"metadata":{"id":"TjzpguOlerjz"}},{"cell_type":"markdown","source":["KoGPT2 (한국어 GPT-2) Ver 2.0 : 한국어 자연어 처리를 위해 특별히 사전 훈련된 GPT-2 모델\n","- 한국어 특화: KoGPT2는 한국어 텍스트 데이터를 바탕으로 사전 훈련되었기 때문에 한국어 문법과 어휘에 대한 이해도가 높습니다. 이를 통해 한국어 자연어 이해 및 생성 태스크에서 뛰어난 성능을 보입니다.\n","- 다양한 응용 가능: KoGPT2는 텍스트 생성, 문장 완성, 챗봇 개발, 문서 요약, 번역 등 다양한 자연어 처리 작업에 활용될 수 있습니다.\n","사전 훈련된 모델: KoGPT2는 대규모 한국어 데이터셋을 사용하여 사전 훈련된 모델을 제공합니다. 사용자는 이 사전 훈련된 모델을 바탕으로 특정 태스크에 대한 미세 조정(fine-tuning)을 수행하여 사용할 수 있습니다.(레이블이 있는 데이터로 해야함)\n","\n","- Transformer 기반: KoGPT2도 GPT-2와 마찬가지로 Transformer 아키텍처를 기반으로 합니다. Transformer는 어텐션 메커니즘(attention mechanism)을 사용하여 텍스트의 다양한 부분 사이의 관계를 효과적으로 학습할 수 있습니다.\n","- KoGPT2 모델은 transformers 라이브러리를 통해 쉽게 사용할 수 있으며, SKT에서 제공하는 skt/kogpt2-base-v2와 같은 모델 체크포인트를 활용하여 특정 태스크에 미세 조정할 수 있습니다.\n","\n","\n","https://github.com/SKT-AI/KoGPT2"],"metadata":{"id":"HwZxA0WBUZYP"}},{"cell_type":"markdown","source":["### pad, eos (end-of-sequence), bos (beginning-of-sequence) 옵션\n","\n","- 토크나이저에서의 사용은 데이터를 모델이 처리할 수 있는 형태로 준비하는 데 중점을 둡니다. 이는 입력 데이터의 전처리와 관련이 있습니다. 토크나이저는 텍스트를 모델이 이해할 수 있는 형태, 즉 토큰 ID의 시퀀스로 변환하는 역할을 합니다. 여기서 pad, eos, bos 토큰은 다음과 같은 목적으로 사용됩니다:\n","  - pad_token: 모델이 배치 처리를 할 때, 모든 입력 시퀀스가 동일한 길이를 갖도록 만듭니다. 짧은 시퀀스는 pad_token으로 채워집니다.\n","  - eos_token과 bos_token: 텍스트 시퀀스의 시작과 끝을 명시합니다. 이는 모델이 문장의 경계를 인식하는 데 도움을 주며, 특히 다양한 자연어 처리 작업에서 중요한 정보입니다.\n","- 모델에서의 사용은 모델의 동작과 출력의 특성을 제어하는 데 중요합니다. 이는 모델의 내부 동작과 직접적으로 관련이 있습니다. 모델은 토크나이징된 입력을 받아 자연어 처리 작업을 수행합니다 (예: 텍스트 생성, 번역, 분류 등). 모델에서 pad, eos, bos 토큰을 사용하는 것은 다음과 같은 목적을 가집니다:\n","  - pad_token_id: 주로 모델이 손실을 계산할 때 사용됩니다. 모델은 패딩된 부분을 무시하고 실제 유용한 데이터에만 집중해야 합니다.\n","  - eos_token_id와 bos_token_id: 텍스트 생성 작업에서, 모델이 문장을 시작하거나 종료할 시점을 결정하는 데 사용됩니다. 예를 들어, eos_token_id는 모델이 문장을 종료해야 할 때 사용되며, bos_token_id는 생성 작업의 시작점으로 사용될 수 있습니다."],"metadata":{"id":"STLGTAQMYR44"}},{"cell_type":"code","source":["from transformers import PreTrainedTokenizerFast\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n","pad_token='<pad>')"],"metadata":{"id":"UZLOCFpzef6V","colab":{"base_uri":"https://localhost:8080/","height":124,"referenced_widgets":["c7aee878aefc4ca8bb9bacd3d14f98a6","42a69ee07eac477db0358d220610a0cb","8ae67b6d46204cc6b37e58fe3585cd5a","8da52c0aa209442b887067ea9f8c644d","ad606911a98a4918bd6cce9b9306e099","4eeffa709b854b4ea049686c965e5a2a","bf139c111a8e42dc84307526ee64b1e5","6594ed47a26e43a8bc7e262245ef514c","d92a911759234200b45fac4e288933fc","5660a55ddaba46eb9ebeb2e7f4467027","c282ae71943b4ff29b5f7dc120f5be24"]},"executionInfo":{"status":"ok","timestamp":1708479476364,"user_tz":-540,"elapsed":2341,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"1d6e0a80-0b93-49e7-f59f-5566dde65ce5"},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7aee878aefc4ca8bb9bacd3d14f98a6"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n","The class this function is called from is 'PreTrainedTokenizerFast'.\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import GPT2LMHeadModel\n","\n","model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n","text = '근육이 커지기 위해서는'\n","# 텍스트를 토큰화하고 필요한 정보 얻기\n","encoded_input = tokenizer(text, return_tensors='pt', padding=\"max_length\", max_length=12, truncation=True)\n","\n","encoded_input.keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J47Rv64JrXv6","executionInfo":{"status":"ok","timestamp":1708479482982,"user_tz":-540,"elapsed":4068,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"01fcfd04-2b28-45f9-931f-f2abe2e50f24"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# input_ids와 attention_mask 확인\n","input_ids = encoded_input['input_ids']\n","attention_mask = encoded_input['attention_mask']\n","\n","print(\"input_ids:\", input_ids)\n","print(\"attention_mask:\", attention_mask)\n","# 3은 특수 토큰(보통 3미만은 특수 토큰)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k7fPuGCir3uH","executionInfo":{"status":"ok","timestamp":1708479485916,"user_tz":-540,"elapsed":2,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"a8ebe0f4-f0e4-4b95-ffa6-d8a7928af6aa"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["input_ids: tensor([[33245, 10114, 12748, 11357,     3,     3,     3,     3,     3,     3,\n","             3,     3]])\n","attention_mask: tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])\n"]}]},{"cell_type":"markdown","source":["gpt model 생성"],"metadata":{"id":"moO32Dooviqp"}},{"cell_type":"code","source":["import torch\n","from transformers import GPT2LMHeadModel\n","\n","model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n","text = '근육이 커지기 위해서는'\n","\n","input_ids = tokenizer.encode(text, return_tensors='pt')\n","gen_ids = model.generate(input_ids,\n","                           max_length=128,\n","                           repetition_penalty=2.0,\n","                           pad_token_id=tokenizer.pad_token_id,\n","                           eos_token_id=tokenizer.eos_token_id,\n","                           bos_token_id=tokenizer.bos_token_id,\n","                           use_cache=True,\n","                          #  do_sample=False, # Flase로 설정하여 모델이 가장 높은 확률을가진 다음 토큰을 선택, 결과가 결정론적이 되게 함.\n","                           do_sample=True,\n","                           temperature = 0.01 # 하이퍼파라미터..\n","                         )\n","generated = tokenizer.decode(gen_ids[0])\n","print(generated)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F29Sz9VNhghn","executionInfo":{"status":"ok","timestamp":1708481132145,"user_tz":-540,"elapsed":13197,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"8cbb4dc5-ef45-4245-e7aa-42c495dfbd1e"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["근육이 커지기 위해서는 무엇보다 규칙적인 생활습관이 중요하다.\n","특히, 아침식사는 단백질과 비타민이 풍부한 과일과 채소를 많이 섭취하는 것이 좋다.\n","또한 하루 30분 이상 충분한 수면을 취하는 것도 도움이 된다.\n","아침 식사를 거르지 않고 규칙적으로 운동을 하면 혈액순환에 도움을 줄 뿐만 아니라 신진대사를 촉진해 체내 노폐물을 배출하고 혈압을 낮춰준다.\n","운동은 하루에 10분 정도만 하는 게 좋으며 운동 후에는 반드시 스트레칭을 통해 근육량을 늘리고 유연성을 높여야 한다.\n","운동 후 바로 잠자리에 드는 것은 피해야 하며 특히 아침에 일어나면 몸이 피곤해지기 때문에 무리하게 움직이면 오히려 역효과가 날 수도 있다.\n","운동을\n"]}]},{"cell_type":"markdown","metadata":{"id":"IzWd3u-vXz9c"},"source":["### BERT 입력값 만들기 - 기 작성한 wordpiece 활용\n","\n","- BERT (Bidirectional Encoder Representations from Transformers)는 다양한 자연어 처리(NLP) 작업에서 사용되는 인기 있는 모델. BertTokenizer는 특히 BERT 모델에 적합한 텍스트 토큰화 도구\n","- BERT 모델 입력값을 만들려면 자신의 구글 드라이브 경로(`/gdrive/My Drive/nlpbook/wordpiece`)에 워드피스 어휘집합 구축 결과(`vocab.txt`)가 있어야 한다. 이미 만들어 놓은 워드피스 어휘집합을 포함한 BERT 토크나이저를 `tokenizer_bert`라는 변수로 선언한다."]},{"cell_type":"code","metadata":{"id":"JTFeKKzJX-O7","executionInfo":{"status":"ok","timestamp":1708479514427,"user_tz":-540,"elapsed":631,"user":{"displayName":"한정현","userId":"04742589720279403748"}}},"source":["# BertTokenizer는 BERT 모델이 이해할 수 있는 형태로 텍스트를 토큰화하는 데 사용\n","from transformers import BertTokenizer\n","tokenizer_bert = BertTokenizer.from_pretrained(  # 사전 훈련된 토크나이저 로드\n","    \"/gdrive/My Drive/nlpbook/wordpiece\", # 만들어놓은 단어사전을 넣어서 토크나이저..\n","    do_lower_case=False, # 토크나이저가 텍스트를 소문자로 변환하지 않도록 지정\n",")"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9pcxOXJxaER5"},"source":["예시 문장 세 개를 각각 토큰화해보겠습니다."]},{"cell_type":"code","metadata":{"id":"c9AzWXcvaJKH","executionInfo":{"status":"ok","timestamp":1708479523015,"user_tz":-540,"elapsed":495,"user":{"displayName":"한정현","userId":"04742589720279403748"}}},"source":["sentences = [\n","    \"아 더빙.. 진짜 짜증나네요 목소리\",\n","    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n","    \"별루 였다..\",\n","]\n","tokenized_sentences = [tokenizer_bert.tokenize(sentence) for sentence in sentences]"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xJhOHq-maNLw"},"source":["토큰화 결과를 확인합니다."]},{"cell_type":"code","metadata":{"id":"HdzBTLFPaPFU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708478258543,"user_tz":-540,"elapsed":2,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"6de75076-8853-40a8-b085-72023e37f93b"},"source":["print(tokenized_sentences)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[['아', '더빙', '.', '.', '진짜', '짜증나', '##네요', '목소리'], ['흠', '.', '.', '.', '포스터', '##보고', '초딩', '##영화', '##줄', '.', '.', '.', '.', '오버', '##연기', '##조차', '가볍', '##지', '않', '##구나'], ['별루', '였다', '.', '.']]\n"]}]},{"cell_type":"markdown","metadata":{"id":"cMxTUoowYHb2"},"source":["이번 배치의 크기가 3이라고 가정하고 이번 배치의 입력값을 만들어 보겠습니다."]},{"cell_type":"code","metadata":{"id":"G_sFosQjYIE3","executionInfo":{"status":"ok","timestamp":1708479658572,"user_tz":-540,"elapsed":656,"user":{"displayName":"한정현","userId":"04742589720279403748"}}},"source":["sentences = [\n","    \"아 더빙.. 진짜 짜증나네요 목소리\",\n","    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n","    \"별루 였다..\",\n","]\n","batch_inputs = tokenizer_bert(\n","    sentences,\n","    padding=\"max_length\", # max_length 매개변수로 지정된 최대 길이와 동일한 길이로 채워지도록 보장\n","    max_length=12, # 토큰화된 각 시퀀스가 ​​가져야 하는 고정 길이를 설정\n","    truncation=True, # 토크나이저가 max_length를 초과하는 시퀀스를 자를 수 있도록 허용\n",")"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ohNW5R3zYOW5"},"source":["`batch_inputs`의 내용을 확인해보겠습니다."]},{"cell_type":"code","metadata":{"id":"RZiMSoaKYTUX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708479662820,"user_tz":-540,"elapsed":1864,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"586096a0-62a9-433c-abc1-6be3faf90605"},"source":["# # BERT 모델 세 가지의 입력값\n","batch_inputs.keys()"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["- [CLS] 토큰은 \"분류\"를 나타내며 모든 입력 시퀀스의 첫 번째 토큰으로 사용. BERT가 분류 작업(예: 감정 분석, 의도 감지)에 사용되는 경우 이 토큰의 표현은 분류 작업의 집계 시퀀스 표현으로 사용된다. 본질적으로 이는 전체 입력 시퀀스의 의미를 요약하는 역할\n","- [SEP] 토큰은 \"구분 기호\"를 나타내며 입력 내에서 개별 세그먼트를 구분하는 데 사용. 이는 질문 답변(모델이 질문과 맥락을 구별해야 하는 경우) 또는 문장 쌍 작업(예: 모델이 두 문장 간의 관계를 결정하는 자연어 추론)과 같은 여러 입력 시퀀스가 ​​포함된 작업에 특히 중요\n","- 예를 들어 문장 쌍 분류 작업에서 입력은 [CLS] 문장 1 [SEP] 문장 2 [SEP]와 같을 수 있으며 [SEP] 토큰은 첫 번째 문장의 끝을 표시하고 구분"],"metadata":{"id":"fYwQY7_Yw6l9"}},{"cell_type":"code","metadata":{"id":"4pWaGuSdYZGR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708479666977,"user_tz":-540,"elapsed":1056,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"481155ff-ad05-4299-f757-039dfc2d3f9a"},"source":["# 토큰 인덱스 시퀀스\n","# 문장 앞에 2, 끝에 3이 붙는 것은 각각 [CLS], [SEP] 라는 토큰에 대응하는 인덱스\n","batch_inputs['input_ids']"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[2, 620, 2631, 16, 16, 1993, 3678, 1990, 3323, 3, 0, 0],\n"," [2, 997, 16, 16, 16, 2609, 2045, 2796, 1981, 1086, 16, 3],\n"," [2, 3274, 9508, 16, 16, 3, 0, 0, 0, 0, 0, 0]]"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"djtA4xkIYbOk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708479670383,"user_tz":-540,"elapsed":1329,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"8cbce85b-20aa-4e34-f4b0-25f6a6a5ce78"},"source":["# BERT의 attention_mask는 GPT와 마찬가지로 일반 토큰이 자리한 곳(1)과 패딩 토큰이 자리한 곳(0)을 구분\n","batch_inputs['attention_mask']"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n"," [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n"," [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"cVR15VXZYoTZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708479691438,"user_tz":-540,"elapsed":498,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"e6922ea9-6a9f-4666-c885-aba54ad0ddb1"},"source":["# token_type_ids는 세그먼트(segment)에 해당하는 것으로 모두 0\n","# BERT 모델은 기본적으로 문서(혹은 문장) 2개를 입력받는데, token_type_ids로 구분.\n","# 첫 번째 세그먼트(문서 혹은 문장)에 해당하는 token_type_ids는 0, 두 번째 세그먼트는 1\n","batch_inputs['token_type_ids']"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["### 사전 훈련된 KoBERT 모델 사용"],"metadata":{"id":"vvkD4VaYpG9R"}},{"cell_type":"code","source":["from transformers import BertTokenizer\n","\n","# KoBERT 모델의 사전 훈련된 토크나이저 로드\n","tokenizer_kobert = BertTokenizer.from_pretrained(\n","    \"monologg/kobert\",  # Hugging Face 모델 허브에서 KoBERT 모델을 지정\n","    do_lower_case=False,  # 토크나이저가 텍스트를 소문자로 변환하지 않도록 지정\n",")\n","\n","# 사용 예시\n","text = \"한국어 모델을 사용하여 텍스트 처리를 해봅시다.\"\n","input_ids = tokenizer_kobert.encode(text, add_special_tokens=True)\n","print(input_ids)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":131,"referenced_widgets":["5c579f2e85034b54958eebe0aef48e16","de6194770cf3442eaf08ded83794ddc8","f6fa16bc82aa411995b0b37d4e6ea89d","9dc47c629b5249bcb3b434b820f79289","ed082eccba74464391f33122f2d08fa2","c0d3c00069a74e438798e563d57494f8","5bb46b1fd3944cab917aa6807f714b23","7239f427ffe04aeba97e52ec7a214e44","1c85fa68214d473c8f76cffcaf299fd8","8b4733361c8042c3836962a0eb7c5fce","b9cac00b8fea4cfc8b6b2d94c7117862","4d1e4569e2554ec3837e3ad7a3bef347","14f763ec82ac43afa2e45f1df9c5c429","6fe1ed43d100432db5cdb72431763ed3","9915fea7665a45a680ddd3b42112b82a","b149025e36fe4605807e3ec0d3296d00","47206da8b3744a12a948640f12756893","07f2ebcaa3f74275b78fcdcd8a1876ba","a311d52ccfe6443ca4372d128a665108","63a276a454194391860b2261163408c3","db11c048536b4d8296afcf3249c4a923","803b3620f4c74d0f8e6617f7062d50b1","9c2175fba71b46189968f0db335c5d9d","10cf88d20db34799890b96cc7cea0e8d","67b98310133545d7944a586a15ad1dff","ff184e2ac93948899137c27b786e5902","a5caeabc8ba549568cb7a5e3a38ac10c","0523adc87bd24f44aa12106c0eb9511c","57d9aa7b4c154858acc777a55674f2b2","4d709eedf9e444a2ba663fe2c2bcb94c","403835f603074ff9b5780b817f569a9e","f87b5321878e44819d156200d235b2a7","05683719d1aa4e8eb0bd420e951f9e57"]},"id":"RzI_QA18oVWx","executionInfo":{"status":"ok","timestamp":1708431893937,"user_tz":-540,"elapsed":1196,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"e3b28dd9-2dc4-4e62-bb31-0036b4ec51fe"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/77.8k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c579f2e85034b54958eebe0aef48e16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d1e4569e2554ec3837e3ad7a3bef347"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/426 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c2175fba71b46189968f0db335c5d9d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[2, 0, 0, 0, 0, 0, 0, 54, 3]\n"]}]},{"cell_type":"code","source":["sentences = [\n","    \"아 더빙.. 진짜 짜증나네요 목소리\",\n","    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n","    \"별루 였다..\",\n","]\n","tokenized_sentences = [tokenizer_kobert.tokenize(sentence) for sentence in sentences]\n","print(tokenized_sentences)\n","# out of vacab -> UNK라는 특수 토큰으로..(vacab에 없음)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bPNB78Ocolx8","executionInfo":{"status":"ok","timestamp":1708431907633,"user_tz":-540,"elapsed":330,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"91078ade-df05-47f1-be91-8b112329a72c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['아', '[UNK]', '.', '.', '진짜', '[UNK]', '[UNK]'], ['흠', '.', '.', '.', '[UNK]', '[UNK]', '.', '.', '.', '.', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '였다', '.', '.']]\n"]}]},{"cell_type":"code","source":["sentences = [\n","    \"아 더빙.. 진짜 짜증나네요 목소리\",\n","    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n","    \"별루 였다..\",\n","]\n","batch_inputs = tokenizer_kobert(\n","    sentences,\n","    padding=\"max_length\", # max_length 매개변수로 지정된 최대 길이와 동일한 길이로 채워지도록 보장\n","    max_length=12, # 토큰화된 각 시퀀스가 ​​가져야 하는 고정 길이를 설정\n","    truncation=True, # 토크나이저가 max_length를 초과하는 시퀀스를 자를 수 있도록 허용\n","    return_tensors='pt'\n",")\n","\n","batch_inputs.keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2zJQ2Hl8pQkh","executionInfo":{"status":"ok","timestamp":1708432020377,"user_tz":-540,"elapsed":335,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"7f645c0b-f167-47b8-90b7-621756a2188d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["print(batch_inputs.input_ids)\n","print(batch_inputs.token_type_ids)\n","print(batch_inputs.attention_mask)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dfBbj3zwp3Hp","executionInfo":{"status":"ok","timestamp":1708432023779,"user_tz":-540,"elapsed":372,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"3c9a4030-6a97-4e02-daee-806d070097e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[   2, 6797,    0,   54,   54, 7347,    0,    0,    3,    1,    1,    1],\n","        [   2, 7989,   54,   54,   54,    0,    0,   54,   54,   54,   54,    3],\n","        [   2,    0, 6946,   54,   54,    3,    1,    1,    1,    1,    1,    1]])\n","tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n","tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])\n"]}]}]}