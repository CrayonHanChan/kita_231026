{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cVeIjpS6eZds","executionInfo":{"status":"ok","timestamp":1708046560684,"user_tz":-540,"elapsed":16598,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"d3b7c9fd-9ccc-41fe-b263-d35fe16ca1ea"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["Q. mnist 데이터 셋에 대해서 Pytorch를 적용하여 모델 구성 변경, 조기 학습 중단을 적용하여 학습하고 Best model을 저장한 후 다시 불러와서 테스트 데이터로 평가한 결과를 출력하세요."],"metadata":{"id":"YmMFlzh80O0B"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader, random_split\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","\n","train_size = int(0.8 + len(train_dataset))\n","val_size = len(train_dataset) - train_size\n","train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n","\n","trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","valloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n","testloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","class MyModel(nn.Module):\n","  def __init__(self):\n","      super(MyModel, self).__init__()\n","      self.conv1 = nn.Conv2d(1, 20, 5)\n","      self.pool = nn.MaxPool2d(2, 2)\n","      self.flatten = nn.Flatten()\n","      self.fc1 = nn.Linear(2880, 50)\n","      self.fc2 = nn.Linear(50, 10)\n","\n","  def forward(self, x):\n","      x = self.pool(F.relu(self.conv1(x)))\n","      x = self.flatten(x)\n","      x = F.relu(self.fc1(x))\n","      x = self.fc2(x)\n","      return x\n","\n","model = MyModel()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","best_val_loss = float('inf')\n","patience, trials = 5.0\n","num_epochs = 20\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in trainloader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","val_loss = 0.0\n","model.eval()\n","with torch.no_grad():\n","    for inputs, labels in valloader:\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        val_loss += loss.item()\n","\n","val_loss /= len(valloader)\n","print(f'Epoch {epoch+1}, Train Loss: {running_loss / len(trainloader)}, Val Loss: {val_loss}')\n","\n","if val_loss < best_val_loss:\n","    print(f'Validation Loss Decreased({best_val_loss:.6f}--->{val_loss:.6f}) \\t Saving The Model')\n","    best_val_loss = val_loss\n","    trials = 0\n","    save_path = \"\"\n","    torch.save(model.state_dict(), save_path)\n","else:\n","    trials += 1\n","    if trials >= patience:\n","        print(\"Early stopping triggered\")\n","        break\n","\n","model.load_state_dict(torch.load(save_path))\n","\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for inputs, labels in testloader:\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f'Accuracy on the 10000 test images: {accuracy}%')"],"metadata":{"id":"90mFBmTWRC4N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader, random_split\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","# from torchsummary import summary # 지금 여기서는 필요 없어서 안해도 됨\n","\n","# MNIST 데이터셋을 위한 전처리 과정 정의\n","# 모델의 학습 효율성을 향상시키고, 일반적으로 신경망에서 더 나은 성능을 얻기 위해 널리 사용\n","transform = transforms.Compose([\n","    transforms.ToTensor(),  # 높이, 너비, 채널(예: RGB)의 3차원 배열을 0과 1 사이의 값으로 스케일링된 텐서로 변경\n","    transforms.Normalize((0.5,), (0.5,))  # 이미지를 평균 0.5, 표준편차 0.5로 정규화하여 [-1, 1] 범위로 조정\n","]) # 평균 0 표준편차 1 로 하면 아마 [-3, 3] 범위\n","\n","# MNIST 데이터셋 로드\n","# transform=transform: 이미지에 적용할 전처리 과정을 지정\n","train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","# data디렉터리에 저장(ls로확인가능), 학습이냐아니냐, 다운로드 할거고, 텐서로 바꾸고 -1 부터 1 범위로 조정 하겠다.\n","\n","# 훈련 데이터셋을 훈련 및 검증 세트로 분할\n","train_size = int(0.8 * len(train_dataset))  # 훈련 세트 크기를 전체의 80%로 설정 # 개수니까 정수로 해야해서int.(부동소수점 버림)\n","val_size = len(train_dataset) - train_size  # 검증 세트 크기 계산\n","train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])  # 분할 실행\n","\n","# DataLoader는 데이터셋에서 미니 배치를 자동으로 생성하고, 이를 모델 학습이나 평가에 이용할 수 있게 해주는 유틸리티\n","# 여기서 데이터포인트 단위로 되어있어서 shuffle=True 를 trainloader에서 하는데 .. 검증이나 테스트는 평가하기 위한거니까 할 필요 없고.\n","# batch_size =64 -> 여기가 안에 들어갈 데이터포인트들이 셔플되는것임\n","trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True) # 데이터를 로드하기 전에 데이터셋을 무작위로 섞기\n","valloader = DataLoader(val_dataset, batch_size=64, shuffle=False) # 데이터의 순서가 성능 평가에 영향을 미치지 않는다.\n","testloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","# 모델 아키텍처 정의\n","# MyModel 클래스는 1채널 그레이스케일 이미지를 입력으로 받아, 10개의 출력 클래스를 가지는 분류 문제에 사용될 수 있는 구조\n","class MyModel(nn.Module):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 20, 5)  # 컨볼루션 레이어 정의 (입력 채널 1, 출력 채널 20, 커널 크기 5) # 흑백 인데 5by5로...\n","        self.pool = nn.MaxPool2d(2, 2)  # 맥스풀링 레이어 정의 (2x2 풀링) # 4개짜리\n","        self.flatten = nn.Flatten()  # 텐서 평탄화\n","        self.fc1 = nn.Linear(2880, 50)  # 완전 연결 레이어 (입력 크기 2880, 출력 크기 50)\n","        self.fc2 = nn.Linear(50, 10)  # 출력 레이어 (클래스 수 10)\n","\n","    def forward(self, x): # 입력 데이터 x가 모델을 통과할 때 수행되는 계산을 정의\n","        x = self.pool(F.relu(self.conv1(x)))  # ReLU 활성화 함수 적용 후 맥스풀링 # relu비선형 하고 pool에 넣어서 맥스풀링\n","        x = self.flatten(x)  # 평탄화\n","        x = F.relu(self.fc1(x))  # ReLU 활성화 함수 적용 # fc1 완전연결층에 집어넣기.\n","        x = self.fc2(x)  # 최종 출력\n","        return x\n","\n","model = MyModel()  # 모델 인스턴스(객체) 생성\n","\n","# 손실 함수 및 최적화 알고리즘 지정\n","# 모멘텀이 지역 최솟값에서 벗어나는데 도움이 될 수 있는 이유는, 관성의 효과로 인해 모델이 작은 지역 최솟값에서 쉽게 \"빠져나와\"\n","# 더 낮은 손실을 가지는 전역 최솟값 또는 더 큰 지역 최솟값을 찾아갈 가능성이 높아지기 때문(0은 관성이 없고 1은 크다.)\n","criterion = nn.CrossEntropyLoss()  # 크로스 엔트로피 손실 함수\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # SGD(확률적경사하강법) 최적화 알고리즘.\n","# lr 설명. 새 가중치=현재 가중치−(학습률(=0.01)×가중치의 기울기)\n","# lr은 학습률을 의미. 학습률이 너무 크면 최적점을 지나쳐 발산할 수 있고, 너무 작으면 학습이 매우 느려지거나 지역 최소값에 갇힐 수 있음\n","# momentum은 탈력을 받아서 실지적으로 가장 낮은지역이아닌데 빠지지 않고 더 진행할수 있도록 하는것.\n","# (탄력도) 지역의 최소점을 방지하는것. 더 학습을 진행 할 수 있게함\n","\n","# 모델 훈련\n","best_val_loss = float('inf')  # 검증 손실을 추적하기 위한 변수 초기화\n","patience, trials = 5, 0  # 조기 종료 기준 설정\n","num_epochs = 20  # 에폭 수 설정\n","for epoch in range(num_epochs):\n","    model.train()  # 모델을 훈련 모드로 설정\n","    running_loss = 0.0\n","    for inputs, labels in trainloader:\n","        optimizer.zero_grad()  # 그래디언트 초기화\n","        outputs = model(inputs)  # 모델을 통한 순전파\n","        loss = criterion(outputs, labels)  # 손실 계산\n","        loss.backward()  # 역전파\n","        optimizer.step()  # 파라미터 업데이트\n","        running_loss += loss.item()\n","\n","    # 검증 단계\n","    val_loss = 0.0\n","    model.eval()  # 모델을 평가 모드로 설정\n","    with torch.no_grad(): # 검증단계에선 no_grad()를 씀\n","        for inputs, labels in valloader:\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels) # 아웃풋과 실제 레이블을 비교해서 손실을 구하고\n","            val_loss += loss.item()\n","\n","    val_loss /= len(valloader)  # 평균 검증 손실 계산\n","    print(f'Epoch {epoch+1}, Train Loss: {running_loss / len(trainloader)}, Val Loss: {val_loss}')\n","\n","    # 검증 손실이 개선되었는지 확인하고 모델 저장\n","    if val_loss < best_val_loss:\n","        print(f'Validation Loss Decreased({best_val_loss:.6f}--->{val_loss:.6f}) \\t Saving The Model')\n","        best_val_loss = val_loss\n","        trials = 0\n","        save_path = \"/content/drive/MyDrive/hjh_kita_directory/Github/kita_231026/m6_dl/data/model/best_model.pth\"\n","        torch.save(model.state_dict(), save_path)  # 모델 저장\n","    else:\n","        trials += 1\n","        if trials >= patience:  # 조기 종료 조건 충족 확인\n","            print(\"Early stopping triggered\")\n","            break\n","\n","# 최고의 모델을 불러와서 평가\n","model.load_state_dict(torch.load(save_path))  # 모델 상태 불러오기\n","\n","# 모델 평가\n","correct = 0\n","total = 0\n","with torch.no_grad():  # 그래디언트 계산 비활성화\n","    for inputs, labels in testloader:\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs, 1) # predicted는 최대값의 인덱스를 의미. max_values, max_indices = torch.max(t, dim=1)\n","        total += labels.size(0) # 0 은 배치에 포함된 샘플 수\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total  # 정확도 계산\n","print(f'Accuracy on the 10000 test images: {accuracy}%')  # 정확도 출력"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1zGy0vU2eSFE","executionInfo":{"status":"ok","timestamp":1708047272797,"user_tz":-540,"elapsed":462225,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"a39c696a-6960-4cb8-e8c5-7c92abfd5bf7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 143978665.01it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 38975448.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 48408916.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 22598492.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Epoch 1, Train Loss: 0.2484357401430607, Val Loss: 0.08510615367026246\n","Validation Loss Decreased(inf--->0.085106) \t Saving The Model\n","Epoch 2, Train Loss: 0.07484213338637104, Val Loss: 0.06409525296878704\n","Validation Loss Decreased(0.085106--->0.064095) \t Saving The Model\n","Epoch 3, Train Loss: 0.0526699892229711, Val Loss: 0.05646144534012877\n","Validation Loss Decreased(0.064095--->0.056461) \t Saving The Model\n","Epoch 4, Train Loss: 0.042359661794888474, Val Loss: 0.0661014606631262\n","Epoch 5, Train Loss: 0.03267651420583328, Val Loss: 0.05865780809154843\n","Epoch 6, Train Loss: 0.028265214133386812, Val Loss: 0.048932730132946745\n","Validation Loss Decreased(0.056461--->0.048933) \t Saving The Model\n","Epoch 7, Train Loss: 0.023692015175901665, Val Loss: 0.05130573351368397\n","Epoch 8, Train Loss: 0.01928090136599106, Val Loss: 0.048031457651535325\n","Validation Loss Decreased(0.048933--->0.048031) \t Saving The Model\n","Epoch 9, Train Loss: 0.015284230755642057, Val Loss: 0.04896069908531028\n","Epoch 10, Train Loss: 0.011785281995932262, Val Loss: 0.04604154236159879\n","Validation Loss Decreased(0.048031--->0.046042) \t Saving The Model\n","Epoch 11, Train Loss: 0.009981636713986518, Val Loss: 0.05257772632215532\n","Epoch 12, Train Loss: 0.00887595211577718, Val Loss: 0.05243184878058178\n","Epoch 13, Train Loss: 0.007531149654523082, Val Loss: 0.049659948391482876\n","Epoch 14, Train Loss: 0.005874030641871892, Val Loss: 0.04798044152969251\n","Epoch 15, Train Loss: 0.0038404668134171516, Val Loss: 0.045622543542815544\n","Validation Loss Decreased(0.046042--->0.045623) \t Saving The Model\n","Epoch 16, Train Loss: 0.0028661585085731833, Val Loss: 0.04609149836391651\n","Epoch 17, Train Loss: 0.0019688031784511014, Val Loss: 0.05019268329991238\n","Epoch 18, Train Loss: 0.0016022787665424404, Val Loss: 0.049482953163373596\n","Epoch 19, Train Loss: 0.0012166483650216833, Val Loss: 0.04977715335767824\n","Epoch 20, Train Loss: 0.0008945710804079378, Val Loss: 0.050464214383093856\n","Early stopping triggered\n","Accuracy on the 10000 test images: 98.91%\n"]}]},{"cell_type":"markdown","source":["torch.max(t, dim=1) 함수\n","- PyTorch에서 주어진 텐서 t의 각 행(row)에 대한 최대값과 해당 최대값의 인덱스를 찾는 데 사용\n","- dim=1은 함수가 작업을 수행할 차원을 지정"],"metadata":{"id":"sBx2VEU0jHvH"}},{"cell_type":"code","source":["import torch\n","\n","# 예시 텐서\n","t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n","print(t)\n","# 전체 텐서에서 최대값\n","max_val = torch.max(t)\n","print(max_val)  # 값: 9\n","\n","# 각 행의 최대값과 해당 인덱스 찾기\n","max_values, max_indices = torch.max(t, dim=1)\n","print(max_values)  # 값: tensor([3, 6, 9])\n","print(max_indices)  # 인덱스: tensor([2, 2, 2])"],"metadata":{"id":"Of-SNgF6M3Xr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708047324655,"user_tz":-540,"elapsed":3487,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"81734dee-867b-4db7-81c8-d4f76912d57b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 2, 3],\n","        [4, 5, 6],\n","        [7, 8, 9]])\n","tensor(9)\n","tensor([3, 6, 9])\n","tensor([2, 2, 2])\n"]}]},{"cell_type":"markdown","source":["Q. 저장된 모델 best_model.pth를 불러와서 test 데이터로 평가를 수행하세요."],"metadata":{"id":"tGbveUGkht3r"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader, random_split\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchsummary import sunmmary\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","train_size = int(0.8 * len(train_dataset))\n","val_size = len(train_dataset) - train_size\n","train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n","\n","trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","valloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n","testloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","class MyModel(nn.Module):\n","    def __init__"],"metadata":{"id":"h6jEHhK0jfIH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader, random_split\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchsummary import summary\n","\n","# MNIST 데이터셋을 위한 전처리 과정 정의\n","# 모델의 학습 효율성을 향상시키고, 일반적으로 신경망에서 더 나은 성능을 얻기 위해 널리 사용\n","transform = transforms.Compose([\n","    transforms.ToTensor(),  # 높이, 너비, 채널(예: RGB)의 3차원 배열을 0과 1 사이의 값으로 스케일링된 텐서로 변경\n","    transforms.Normalize((0.5,), (0.5,))  # 이미지를 평균 0.5, 표준편차 0.5로 정규화하여 [-1, 1] 범위로 조정\n","])\n","\n","# MNIST 데이터셋 로드\n","# transform=transform: 이미지에 적용할 전처리 과정을 지정\n","train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","\n","# 훈련 데이터셋을 훈련 및 검증 세트로 분할\n","train_size = int(0.8 * len(train_dataset))  # 훈련 세트 크기를 전체의 80%로 설정\n","val_size = len(train_dataset) - train_size  # 검증 세트 크기 계산\n","train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])  # 분할 실행\n","\n","# DataLoader는 데이터셋에서 미니 배치를 자동으로 생성하고, 이를 모델 학습이나 평가에 이용할 수 있게 해주는 유틸리티\n","trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True) # 데이터를 로드하기 전에 데이터셋을 무작위로 섞기\n","valloader = DataLoader(val_dataset, batch_size=64, shuffle=False) # 데이터의 순서가 성능 평가에 영향을 미치지 않는다.\n","testloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","# 모델 아키텍처 정의\n","# MyModel 클래스는 1채널 그레이스케일 이미지를 입력으로 받아, 10개의 출력 클래스를 가지는 분류 문제에 사용될 수 있는 구조\n","class MyModel(nn.Module):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 20, 5)  # 컨볼루션 레이어 정의 (입력 채널 1, 출력 채널 20, 커널 크기 5)\n","        self.pool = nn.MaxPool2d(2, 2)  # 맥스풀링 레이어 정의 (2x2 풀링)\n","        self.flatten = nn.Flatten()  # 텐서 평탄화\n","        self.fc1 = nn.Linear(2880, 50)  # 완전 연결 레이어 (입력 크기 2880, 출력 크기 50)\n","        self.fc2 = nn.Linear(50, 10)  # 출력 레이어 (클래스 수 10)\n","\n","    def forward(self, x): # 입력 데이터 x가 모델을 통과할 때 수행되는 계산을 정의\n","        x = self.pool(F.relu(self.conv1(x)))  # ReLU 활성화 함수 적용 후 맥스풀링\n","        x = self.flatten(x)  # 평탄화\n","        x = F.relu(self.fc1(x))  # ReLU 활성화 함수 적용\n","        x = self.fc2(x)  # 최종 출력\n","        return x\n","\n","model = MyModel()  # 모델 인스턴스 생성\n","\n","# 최고의 모델을 불러와서 평가\n","save_path = \"/content/drive/MyDrive/hjh_kita_directory/Github/kita_231026/m6_dl/data/model/best_model.pth\"\n","model.load_state_dict(torch.load(save_path))  # 모델 상태 불러오기\n","\n","# 모델 평가\n","correct = 0\n","total = 0\n","with torch.no_grad():  # 그래디언트 계산 비활성화\n","    for inputs, labels in testloader:\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs, 1) # max_values, max_indices = torch.max(t, dim=1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total  # 정확도 계산\n","print(f'Accuracy on the 10000 test images: {accuracy}%')  # 정확도 출력"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-me0TfZogO1Z","executionInfo":{"status":"ok","timestamp":1708047614177,"user_tz":-540,"elapsed":5046,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"cad59ccd-5eeb-4186-b9a5-15a5ae5f6001"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on the 10000 test images: 98.91%\n"]}]},{"cell_type":"markdown","source":["Q. 저장된 모델 best_model.pth를 불러와서 epochs를 추가하여 학습한 후 test 데이터로 평가를 수행하고 모델을 best_model1.pth로 저장하세요."],"metadata":{"id":"rjj1JQIpjRo7"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader, random_split\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","\n","# MNIST 데이터셋을 위한 전처리 과정 정의\n","# 모델의 학습 효율성을 향상시키고, 일반적으로 신경망에서 더 나은 성능을 얻기 위해 널리 사용\n","transform = transforms.Compose([\n","    transforms.ToTensor(),  # 높이, 너비, 채널(예: RGB)의 3차원 배열을 0과 1 사이의 값으로 스케일링된 텐서로 변경\n","    transforms.Normalize((0.5,), (0.5,))  # 이미지를 평균 0.5, 표준편차 0.5로 정규화하여 [-1, 1] 범위로 조정\n","])\n","\n","# MNIST 데이터셋 로드\n","# transform=transform: 이미지에 적용할 전처리 과정을 지정\n","train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","\n","# 훈련 데이터셋을 훈련 및 검증 세트로 분할\n","train_size = int(0.8 * len(train_dataset))  # 훈련 세트 크기를 전체의 80%로 설정\n","val_size = len(train_dataset) - train_size  # 검증 세트 크기 계산\n","train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])  # 분할 실행\n","\n","# DataLoader는 데이터셋에서 미니 배치를 자동으로 생성하고, 이를 모델 학습이나 평가에 이용할 수 있게 해주는 유틸리티\n","trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True) # 데이터를 로드하기 전에 데이터셋을 무작위로 섞기\n","valloader = DataLoader(val_dataset, batch_size=64, shuffle=False) # 데이터의 순서가 성능 평가에 영향을 미치지 않는다.\n","testloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","# 모델 아키텍처 정의\n","# ImprovedMyModel 클래스는 1채널 그레이스케일 이미지를 입력으로 받아, 10개의 출력 클래스를 가지는 분류 문제에 사용될 수 있는 구조\n","class ImprovedMyModel(nn.Module):\n","    def __init__(self):\n","        super(ImprovedMyModel, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 20, 5)  # 컨볼루션 레이어 정의 (입력 채널 1, 출력 채널 20, 커널 크기 5)\n","        self.pool = nn.MaxPool2d(2, 2)  # 맥스풀링 레이어 정의 (2x2 풀링)\n","        self.flatten = nn.Flatten()  # 텐서 평탄화\n","        self.fc1 = nn.Linear(2880, 50)  # 완전 연결 레이어 (입력 크기 2880, 출력 크기 50)\n","        self.fc2 = nn.Linear(50, 10)  # 출력 레이어 (클래스 수 10)\n","\n","    def forward(self, x): # 입력 데이터 x가 모델을 통과할 때 수행되는 계산을 정의\n","        x = self.pool(F.relu(self.conv1(x)))  # ReLU 활성화 함수 적용 후 맥스풀링\n","        x = self.flatten(x)  # 평탄화\n","        x = F.relu(self.fc1(x))  # ReLU 활성화 함수 적용\n","        x = self.fc2(x)  # 최종 출력\n","        return x\n","\n","# 모델 아키텍처 인스턴스 생성\n","model = ImprovedMyModel()\n","# 모델 객체 이름이 바뀔순 있지만, 사용자 함수의 가중치 구조를 바꿀순 없음.(물론 고급에서는 바꿈) 바꾸면 오류남. 대신 에폭을 늘리면됨\n","\n","# 저장된 state_dict 불러오기\n","state_dict = torch.load('/content/drive/MyDrive/hjh_kita_directory/Github/kita_231026/m6_dl/data/model/best_model.pth')\n","\n","# 불러온 state_dict를 모델에 로드\n","model.load_state_dict(state_dict)\n","\n","# 모델 구조 출력\n","print(model)\n","\n","# 손실 함수 및 최적화 알고리즘 지정\n","criterion = nn.CrossEntropyLoss()  # 크로스 엔트로피 손실 함수\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # SGD 최적화 알고리즘\n","\n","# epochs 추가 학습을 위한 설정\n","num_additional_epochs = 10\n","for epoch in range(num_additional_epochs):\n","    model.train()  # 모델을 훈련 모드로 설정\n","    running_loss = 0.0\n","    for inputs, labels in trainloader:\n","        optimizer.zero_grad()  # 그래디언트 초기화\n","        outputs = model(inputs)  # 모델을 통한 순전파\n","        loss = criterion(outputs, labels)  # 손실 계산\n","        loss.backward()  # 역전파\n","        optimizer.step()  # 파라미터 업데이트\n","        running_loss += loss.item()\n","    # 여기서는 간단화를 위해 검증 단계는 생략했습니다. 필요에 따라 추가하세요.\n","    print(f'Epoch {epoch+1}, Train Loss: {running_loss / len(trainloader)}')\n","\n","\n","# 모델 평가\n","correct = 0\n","total = 0\n","with torch.no_grad():  # 그래디언트 계산 비활성화\n","    for inputs, labels in testloader:\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total  # 정확도 계산\n","print(f'Accuracy on the 10000 test images: {accuracy}%')  # 정확도 출력\n","\n","import os\n","# Define the save path\n","save_path = '/content/drive/MyDrive/hjh_kita_directory/Github/kita_231026/m6_dl/data/model/best_model1.pth'\n","\n","# Ensure directory exists\n","os.makedirs(os.path.dirname(save_path), exist_ok=True)\n","\n","# 모델의 매개변수(가중치와 편향)만 저장\n","torch.save(model.state_dict(), save_path)\n","print(\"Saved PyTorch Model State to model.pth\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sbWYzaFbjVZJ","executionInfo":{"status":"ok","timestamp":1708048359548,"user_tz":-540,"elapsed":185909,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"4cbd6d56-24c8-4c6b-9d9f-76ed74a31921"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["ImprovedMyModel(\n","  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (fc1): Linear(in_features=2880, out_features=50, bias=True)\n","  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",")\n","Epoch 1, Train Loss: 0.021417105742545878\n","Epoch 2, Train Loss: 0.013603444273083975\n","Epoch 3, Train Loss: 0.009824811068200991\n","Epoch 4, Train Loss: 0.007183098133443612\n","Epoch 5, Train Loss: 0.005815988393776934\n","Epoch 6, Train Loss: 0.0037090883138686573\n","Epoch 7, Train Loss: 0.0021656318516276468\n","Epoch 8, Train Loss: 0.0016981516563312955\n","Epoch 9, Train Loss: 0.0013744766928127016\n","Epoch 10, Train Loss: 0.0011055277176904687\n","Accuracy on the 10000 test images: 98.85%\n","Saved PyTorch Model State to model.pth\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}