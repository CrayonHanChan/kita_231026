{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"15rJx6kK3zO6jjgknqB5UqjdlsyrVGZUR","authorship_tag":"ABX9TyMLai2S1uf8qOgh996yhJjg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### 텍스트의 토큰화\n","- NLP(자연어 처리) 토큰화는 텍스트 분석, 언어 번역, 감정 분석 등과 같은 NLP 작업을 위해 텍스트를 준비하는 프로세스의 기본 단계.\n","- 토큰화에는 텍스트를 단어, 문구, 기호 또는 토큰이라고 하는 기타 의미 있는 요소와 같은 개별 구성 요소로 분해하는 작업이 포함. 토큰은 추가 처리 및 분석을 위한 구성 요소가 된다.\n","- 토큰화의 목적은 텍스트 데이터를 단순화하고 알고리즘이 이해하고 처리할 수 있도록 관리하기 쉽게 만드는 것이다. NLP 작업의 특정 요구 사항에 따라 토큰은 단어, 문장 또는 단어의 일부(예: 어간 또는 하위 단어)일 수도 있다. 이 프로세스는 불필요한 구두점과 공백을 제거하는 데 도움이 되며 데이터 세트 전체에서 일관성을 유지하기 위해 모든 텍스트를 소문자로 변환하는 작업도 포함될 수 있다."],"metadata":{"id":"Iw5g4B9eweJX"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense,Flatten,Embedding\n","from tensorflow.keras.utils import to_categorical\n","from numpy import array\n","\n","# 케라스의 텍스트 전처리와 관련한 함수중 text_to_word_sequence 함수를 불러옵니다.\n","from tensorflow.keras.preprocessing.text import text_to_word_sequence\n","\n","# 전처리할 텍스트를 정합니다.\n","text = '해보지 않으면 해낼 수 없다.'\n","\n","# 해당 텍스트를 토큰화합니다.\n","result = text_to_word_sequence(text)\n","print(\"\\n원문:\\n\", text)\n","print(\"\\n토큰화:\\n\", result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E_gB6Kuhww3d","executionInfo":{"status":"ok","timestamp":1707178996738,"user_tz":-540,"elapsed":4315,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"64fffaed-801e-46de-9b0f-317f351512a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","원문:\n"," 해보지 않으면 해낼 수 없다.\n","\n","토큰화:\n"," ['해보지', '않으면', '해낼', '수', '없다']\n"]}]},{"cell_type":"markdown","source":["- Tokenizer' 클래스는 텍스트를 정수 시퀀스로 변환하도록 설계\n","- fit_on_texts(docs): 이 메소드는 문장 목록(docs)을 인수로 사용하여 token 개체에서 호출된다. 텍스트 목록을 기반으로 내부 어휘를 업데이트하여 토크나이저가 이러한 텍스트로 작업할 수 있도록 준비한다. 말뭉치의 각 고유 단어에 색인을 할당하고 단어 빈도와 같은 다양한 측정항목을 계산하는 작업이 포함된다.\n","- token.word_counts: 토크나이저를 텍스트에 맞춘 후 word_counts는 키가 입력 텍스트에서 발견된 단어이고 값은 각 단어의 발생 횟수인 OrderedDict를 제공한다. 'OrderedDict'를 사용하면 단어가 텍스트에서 처음 나타나는 순서대로 정렬.\n","- token.document_count: 이 속성은 처리된 총 문서(또는 문장) 수를 표시\n","- token.word_docs: word_counts와 유사한 OrderedDict이지만 단어의 빈도 대신 각 단어가 나타나는 문서 수를 표시\n","- token.word_index: 이 속성은 단어를 고유하게 할당된 정수에 매핑하는 OrderedDict를 제공. 모델에는 숫자 입력이 필요하므로 이는 기계 학습 모델의 텍스트를 벡터화하는 데 필요"],"metadata":{"id":"EZcBwin5zB_H"}},{"cell_type":"code","source":["# 단어 빈도수 세기\n","\n","# 전처리하려는 세 개의 문장을 정합니다.\n","docs = ['먼저 텍스트의 각 단어를 나누어 토큰화 합니다.',\n","       '텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.',\n","       '토큰화한 결과는 딥러닝에서 사용할 수 있습니다.',\n","       ]\n","\n","# 토큰화 함수를 이용해 전처리 하는 과정입니다.\n","token = Tokenizer()   # 토큰화 함수 지정\n","token.fit_on_texts(docs)    # 토큰화 함수에 문장 적용\n","\n","# 단어의 빈도수를 계산한 결과를 각 옵션에 맞추어 출력합니다.\n","# Tokenizer()의 word_counts 함수는 순서를 기억하는 OrdereDict 클래스를 사용합니다.\n","print(\"\\n단어 카운트:\\n\", token.word_counts)\n","print(\"\\n문장 카운트:\", token.document_count)\n","print(\"\\n각 단어가 몇 개의 문장에 포함되어 있는가:\\n\", token.word_docs)\n","# token.word_index의 출력 순서는 제공된 텍스트 코퍼스의 각 단어의 빈도에 따라 가장 빈번한 단어부터 가장 빈도가 낮은 단어까지 결정\n","print(\"\\n각 단어에 매겨진 인덱스 값:\\n\", token.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"021S0EFL1iRV","executionInfo":{"status":"ok","timestamp":1707179891575,"user_tz":-540,"elapsed":309,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"5d69802e-3494-4088-d4a2-6cdd477fced8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","단어 카운트:\n"," OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화', 1), ('합니다', 1), ('단어로', 1), ('토큰화해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('토큰화한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n","\n","문장 카운트: 3\n","\n","각 단어가 몇 개의 문장에 포함되어 있는가:\n"," defaultdict(<class 'int'>, {'먼저': 1, '나누어': 1, '합니다': 1, '단어를': 1, '텍스트의': 2, '각': 1, '토큰화': 1, '딥러닝에서': 2, '토큰화해야': 1, '단어로': 1, '인식됩니다': 1, '수': 1, '사용할': 1, '있습니다': 1, '토큰화한': 1, '결과는': 1})\n","\n","각 단어에 매겨진 인덱스 값:\n"," {'텍스트의': 1, '딥러닝에서': 2, '먼저': 3, '각': 4, '단어를': 5, '나누어': 6, '토큰화': 7, '합니다': 8, '단어로': 9, '토큰화해야': 10, '인식됩니다': 11, '토큰화한': 12, '결과는': 13, '사용할': 14, '수': 15, '있습니다': 16}\n"]}]},{"cell_type":"code","source":["# docs = ['검찰이 제시한 혐의 사실 전부를 재판부가 무죄로 판단하면서 이 회장은 검찰 기소 이후 3년 5개월여 만에 시름을 덜게 됐다.',\n","# '검찰 항소로 2심 재판이 진행될 것이란 전망이 나오지만,']\n","\n","docs = ['통신3사는 사전 판매 기간만 해도 공시지원금을 5만~24만원대로 책정했다.',\n","        '이 때문에 ‘짠물’ 지원금이라는 볼멘소리도 나왔다.',\n","        '방송통신위원회는 지난달 31일 통신3사 정책·마케팅 관련 부사장단을 만나 공시지원금 상향을 요청하기도 했다.',\n","        '가장 먼저 화답한 곳은 LG유플러스였다.',\n","        'LG유플러스는 지난 2일 공시지원금을 12만~45만원으로 상향 조정했다.',\n","        'LG유플러스 공시지원금은 당초 5만~23만원대에 불과했다.',\n","        'LG유플러스가 공시지원금을 올리자 업계 안팎에서는 SKT와 KT도 이날 중 상향 조정할 가능성이 크다는 전망이 나왔다.',\n","        '방통위 고시인 ‘지원금 공시 및 게시 방법 등에 관한 세부 기준’은 단말기 지원금 등의 공시 정보를 매주 화·금요일에 변경하도록 규정하고 있기 때문이다.']\n","\n","token = Tokenizer()\n","token.fit_on_texts(docs)\n","\n","print(\"\\n단어 카운트:\\n\", token.word_counts)\n","print(\"\\n문장 카운트:\", token.document_count)\n","print(\"\\n각 단어가 몇 개의 문장에 포함되어 있는가:\\n\", token.word_docs)\n","print(\"\\n각 단어에 매겨진 인덱스 값:\\n\", token.word_index) # 단어별로 인덱스 처리해주는 것. 말뭉치 사전"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aF5H38nD2TAk","executionInfo":{"status":"ok","timestamp":1707180401540,"user_tz":-540,"elapsed":322,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"8eb49e94-3db5-4c5c-c90b-646876a6a4ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","단어 카운트:\n"," OrderedDict([('통신3사는', 1), ('사전', 1), ('판매', 1), ('기간만', 1), ('해도', 1), ('공시지원금을', 3), ('5만', 2), ('24만원대로', 1), ('책정했다', 1), ('이', 1), ('때문에', 1), ('‘짠물’', 1), ('지원금이라는', 1), ('볼멘소리도', 1), ('나왔다', 2), ('방송통신위원회는', 1), ('지난달', 1), ('31일', 1), ('통신3사', 1), ('정책·마케팅', 1), ('관련', 1), ('부사장단을', 1), ('만나', 1), ('공시지원금', 1), ('상향을', 1), ('요청하기도', 1), ('했다', 1), ('가장', 1), ('먼저', 1), ('화답한', 1), ('곳은', 1), ('lg유플러스였다', 1), ('lg유플러스는', 1), ('지난', 1), ('2일', 1), ('12만', 1), ('45만원으로', 1), ('상향', 2), ('조정했다', 1), ('lg유플러스', 1), ('공시지원금은', 1), ('당초', 1), ('23만원대에', 1), ('불과했다', 1), ('lg유플러스가', 1), ('올리자', 1), ('업계', 1), ('안팎에서는', 1), ('skt와', 1), ('kt도', 1), ('이날', 1), ('중', 1), ('조정할', 1), ('가능성이', 1), ('크다는', 1), ('전망이', 1), ('방통위', 1), ('고시인', 1), ('‘지원금', 1), ('공시', 2), ('및', 1), ('게시', 1), ('방법', 1), ('등에', 1), ('관한', 1), ('세부', 1), ('기준’은', 1), ('단말기', 1), ('지원금', 1), ('등의', 1), ('정보를', 1), ('매주', 1), ('화·금요일에', 1), ('변경하도록', 1), ('규정하고', 1), ('있기', 1), ('때문이다', 1)])\n","\n","문장 카운트: 8\n","\n","각 단어가 몇 개의 문장에 포함되어 있는가:\n"," defaultdict(<class 'int'>, {'공시지원금을': 3, '통신3사는': 1, '책정했다': 1, '기간만': 1, '사전': 1, '해도': 1, '5만': 2, '판매': 1, '24만원대로': 1, '이': 1, '지원금이라는': 1, '나왔다': 2, '때문에': 1, '‘짠물’': 1, '볼멘소리도': 1, '만나': 1, '했다': 1, '공시지원금': 1, '부사장단을': 1, '관련': 1, '상향을': 1, '통신3사': 1, '정책·마케팅': 1, '요청하기도': 1, '31일': 1, '방송통신위원회는': 1, '지난달': 1, '먼저': 1, '가장': 1, '곳은': 1, 'lg유플러스였다': 1, '화답한': 1, '12만': 1, '지난': 1, '상향': 2, '2일': 1, '45만원으로': 1, 'lg유플러스는': 1, '조정했다': 1, '23만원대에': 1, 'lg유플러스': 1, '공시지원금은': 1, '당초': 1, '불과했다': 1, '업계': 1, 'kt도': 1, '올리자': 1, '이날': 1, '조정할': 1, '크다는': 1, 'lg유플러스가': 1, '안팎에서는': 1, 'skt와': 1, '가능성이': 1, '전망이': 1, '중': 1, '화·금요일에': 1, '관한': 1, '고시인': 1, '매주': 1, '등의': 1, '기준’은': 1, '등에': 1, '변경하도록': 1, '단말기': 1, '방법': 1, '지원금': 1, '세부': 1, '공시': 1, '규정하고': 1, '게시': 1, '정보를': 1, '방통위': 1, '‘지원금': 1, '때문이다': 1, '및': 1, '있기': 1})\n","\n","각 단어에 매겨진 인덱스 값:\n"," {'공시지원금을': 1, '5만': 2, '나왔다': 3, '상향': 4, '공시': 5, '통신3사는': 6, '사전': 7, '판매': 8, '기간만': 9, '해도': 10, '24만원대로': 11, '책정했다': 12, '이': 13, '때문에': 14, '‘짠물’': 15, '지원금이라는': 16, '볼멘소리도': 17, '방송통신위원회는': 18, '지난달': 19, '31일': 20, '통신3사': 21, '정책·마케팅': 22, '관련': 23, '부사장단을': 24, '만나': 25, '공시지원금': 26, '상향을': 27, '요청하기도': 28, '했다': 29, '가장': 30, '먼저': 31, '화답한': 32, '곳은': 33, 'lg유플러스였다': 34, 'lg유플러스는': 35, '지난': 36, '2일': 37, '12만': 38, '45만원으로': 39, '조정했다': 40, 'lg유플러스': 41, '공시지원금은': 42, '당초': 43, '23만원대에': 44, '불과했다': 45, 'lg유플러스가': 46, '올리자': 47, '업계': 48, '안팎에서는': 49, 'skt와': 50, 'kt도': 51, '이날': 52, '중': 53, '조정할': 54, '가능성이': 55, '크다는': 56, '전망이': 57, '방통위': 58, '고시인': 59, '‘지원금': 60, '및': 61, '게시': 62, '방법': 63, '등에': 64, '관한': 65, '세부': 66, '기준’은': 67, '단말기': 68, '지원금': 69, '등의': 70, '정보를': 71, '매주': 72, '화·금요일에': 73, '변경하도록': 74, '규정하고': 75, '있기': 76, '때문이다': 77}\n"]}]},{"cell_type":"markdown","source":["### 단어의 원-핫 인코딩\n","- word_size = len(token.word_index) + 1: 여기서 token.word_index는 키가 단어이고 값이 해당 고유 정수 인덱스인 사전. token.word_index의 길이는 말뭉치에 있는 고유 단어의 총 개수를 나타낸다. 이 개수에 1을 추가하는 것은 \"0\" 인덱스를 설명하기 위해 NLP에서 일반적인 관행이며는 종종 패딩에 사용되거나 구현에 따라 알 수 없는 단어를 나타낼 수 있습니다.\n","- x = to_categorical(x, num_classes=word_size): 이 줄은 단어를 나타내는 정수 인덱스의 목록 또는 배열이어야 하는 x를 원-핫 인코딩 형식으로 변환한다. to_categorical은 클래스 벡터(정수)를 바이너리 클래스 행렬로 변환하는 함수(일반적으로 Keras의)이며 x의 각 정수에 대해 to_categorical은 1로 설정된 정수 인덱스에 해당하는 위치를 제외하고 모든 요소가 0인 길이 word_size의 벡터를 생성한다.\n","- num_classes=word_size는 총 클래스 수를 지정한다. 이 경우 어휘 크기(word_size)로 설정되어 원-핫 인코딩에 어휘의 모든 단어에 대한 슬롯과 추가 \"0\" 인덱스가 있는지 확인한다."],"metadata":{"id":"UIcXFP2n4gYr"}},{"cell_type":"code","source":["text = \"오랫동안 꿈꾸는 이는 그 꿈을 닮아간다\"\n","token = Tokenizer()\n","token.fit_on_texts([text])\n","# token.fit_on_texts(text) # [] 빼고 해서 하면.. 한 음절씩으로 나뉘는데.. 이렇게는 안함 그 이유는,\n","# Keras Tokenizer의 fit_on_texts 메소드는 일반적으로 문자열의 리스트를 기대하기 때문에, 단일 문자열을 입력하는 것은 적절하지 않다.\n","print(token.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xbUYSegC4jT_","executionInfo":{"status":"ok","timestamp":1707182581518,"user_tz":-540,"elapsed":303,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"2762fcb9-6f8a-4a9b-9b66-1526adb78d5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'오랫동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}\n"]}]},{"cell_type":"code","source":["x=token.texts_to_sequences([text])\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aNnd1n7-8VMx","executionInfo":{"status":"ok","timestamp":1707182582471,"user_tz":-540,"elapsed":2,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"9fdeed79-b3b2-4991-fc47-5ea75322e198"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 2, 3, 4, 5, 6]]\n"]}]},{"cell_type":"code","source":["# 원핫인코딩\n","# 앞에 0은 패딩을 처리하거나 여기에 나와있지 않은 특수 문자나 기호를 처리하기 위해 첫번째 열에는 다 0으로 되어 있음\n","# 인덱스 수\n","# num_classes 는 넣는 클래스 수. 여기에선 6개를 넣고.. +1 되니 컬럼수가 7개.\n","# 1이 있고 나머지 0인 희소행렬이 됨\n","\n","# 첫번째 열에대한 보충 설명,, 문장들을 넣으면 각각의 토큰 수가 다를것임.\n","# 입력할땐 각각 입력 수가 같아야하는데(완전 연결층에 넣을때 하나로만 넣어줌) 토큰화하면 입력사이즈가 다름.\n","# 어떤거는 3개 5개 2개 이러면 5개로 입력사이즈로 하면 3개나2개는 부족함. 그런것을 0으로 채워줌, 그것이 패딩임. 그런것들을 표시해줌\n","\n","# 인덱스 수에 하나를 추가해서 원-핫 인코딩 배열 만들기\n","word_size = len(token.word_index) + 1\n","x = to_categorical(x, num_classes=word_size)\n","print(x)\n","# 이것을 다시 실행하면 원핫인코딩 된것 ->x 에 또 원핫인코딩 되어서 차원이 늘어나는것으로 보임.....\n","# word_size 에 1을 추가하는 이유는(+1 하는 이유). Keras의 Tokenizer를 사용할 때 0의 인덱스를 패딩(padding)을 위해 예약하는 관례 때문"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5dWmNYEu-61g","executionInfo":{"status":"ok","timestamp":1707182583287,"user_tz":-540,"elapsed":2,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"4cbb3687-2020-4a0d-c95b-a3bfaa1b1004"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[0. 1. 0. 0. 0. 0. 0.]\n","  [0. 0. 1. 0. 0. 0. 0.]\n","  [0. 0. 0. 1. 0. 0. 0.]\n","  [0. 0. 0. 0. 1. 0. 0.]\n","  [0. 0. 0. 0. 0. 1. 0.]\n","  [0. 0. 0. 0. 0. 0. 1.]]]\n"]}]},{"cell_type":"markdown","source":["### 텍스트를 읽고 긍정, 부정 예측하기"],"metadata":{"id":"3j-5X30DA05h"}},{"cell_type":"code","source":["# 텍스트 리뷰 자료를 지정합니다.\n","docs = [\"너무 재밌네요\",\"최고예요\",\"참 잘 만든 영화예요\",\"추천하고 싶은 영화입니다\",\"한번 더 보고싶네요\",\\\n","        \"글쎄요\",\"별로예요\",\"생각보다 지루하네요\",\"연기가 어색해요\",\"재미없어요\"]\n","\n","# 긍정 리뷰는 1, 부정 리뷰는 0으로 클래스를 지정합니다.\n","classes = array([1,1,1,1,1,0,0,0,0,0])\n","\n","# 토큰화\n","token = Tokenizer()\n","token.fit_on_texts(docs)\n","print(token.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N2z2P1XGA37y","executionInfo":{"status":"ok","timestamp":1707182960898,"user_tz":-540,"elapsed":317,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"9d706b02-c975-4736-b866-7768a4ddbaa4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n"]}]},{"cell_type":"code","source":["x = token.texts_to_sequences(docs)\n","print(\"\\n리뷰 텍스트, 토큰화 결과:\\n\", x)\n","# 출력해보면.. 제일 많은게 4개임.(참 잘 만든 영화예요). 즉, 제일 많은 4로 맞춰주어야 함"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kZdA-W3nCBMV","executionInfo":{"status":"ok","timestamp":1707182962904,"user_tz":-540,"elapsed":325,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"e1d21cc7-18c8-404c-e5c9-8614b94ae8b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","리뷰 텍스트, 토큰화 결과:\n"," [[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13], [14], [15], [16, 17], [18, 19], [20]]\n"]}]},{"cell_type":"code","source":["# 패딩, 서로 다른 길이의 데이터를 4로 맞추어 줍니다.\n","# padfding = 'pre' : 시퀸스의 길이가 4보다 짧은 경우 앞쪽을 0으로 채워 길이를 4로 맞춘다.\n","padded_x = pad_sequences(x, 4, padding='pre') # default\n","# padded_x = pad_sequences(x, 4, padding='post')\n","\n","print(\"\\n패딩 결과:\\n\", padded_x)\n","\n","# 출력 결과를 확인해 보면.. 너무 재밌네요 는 0이 2개 더 두고, 최고예요 는 0을 3개 더 둠."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fSFpMk2ACJjl","executionInfo":{"status":"ok","timestamp":1707183158232,"user_tz":-540,"elapsed":2,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"d085ced0-a461-4b69-da47-5e6928fe0335"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","패딩 결과:\n"," [[ 0  0  1  2]\n"," [ 0  0  0  3]\n"," [ 4  5  6  7]\n"," [ 0  8  9 10]\n"," [ 0 11 12 13]\n"," [ 0  0  0 14]\n"," [ 0  0  0 15]\n"," [ 0  0 16 17]\n"," [ 0  0 18 19]\n"," [ 0  0  0 20]]\n"]}]},{"cell_type":"code","source":["# padding='pre' (기본값)\n","# padding='pre' 옵션을 사용하면, 시퀀스의 길이가 지정된 최대 길이보다 짧은 경우 앞쪽(시퀀스의 시작 부분)에 0을 삽입하여 길이를 맞춥니다.\n","# 이 방식은 시퀀스의 끝부분(가장 최근의 정보)이 모델에 더 중요할 때 유용하게 사용될 수 있습니다.\n","# padding='post'\n","# padding='post' 옵션을 사용하면, 시퀀스의 길이가 지정된 최대 길이보다 짧은 경우 뒤쪽(시퀀스의 끝 부분)에 0을 삽입하여 길이를 맞춥니다.\n","# 이 방식은 시퀀스의 시작 부분 정보가 모델에 더 중요할 때 유용하게 사용될 수 있습니다.\n","# 예를 들어, 시퀀스 [1, 2, 3]을 최대 길이 4로 패딩하는 경우:\n","\n","# padding='pre' 사용 시: [0, 1, 2, 3]\n","# padding='post' 사용 시: [1, 2, 3, 0]"],"metadata":{"id":"SqPKUhPjPEYb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Q. 주어진 텍스트 데이터셋을 토큰화하고, 각 단어에 고유한 정수 인덱스를 할당하세요. 그런 다음, 각 문장을 정수 시퀀스로 변환하세요.\n","\n","데이터셋:\n","\n","\"I love machine learning.\"<br>\n","\"I love coding in Python.\"<br>\n","\"Machine learning can be fun.\""],"metadata":{"id":"BaavmOueC3cd"}},{"cell_type":"code","source":["# A.\n","docs = [\"I love machine learning.\",\n","\"I love coding in Python.\",\n","\"Machine learning can be fun.\"]\n","\n","token = Tokenizer()\n","token.fit_on_texts(docs)\n","print(\"정수 인덱스 할당:\\n\", token.word_index)\n","\n","seq = token.texts_to_sequences(docs)\n","print(\"\\n토큰화 결과:\\n\", seq)\n","# 5개로 길이 맞춰야함.\n","\n","padded_seq = pad_sequences(seq, 5, padding='pre') # 5 는 maxlen=5 이렇게 써도 됨.\n","print(\"\\n패딩 결과:\\n\", padded_seq)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OuFFewI4EZLh","executionInfo":{"status":"ok","timestamp":1707185239320,"user_tz":-540,"elapsed":4,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"5906650c-9b12-4fa5-c6bb-7c4a7ab0bad5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["정수 인덱스 할당:\n"," {'i': 1, 'love': 2, 'machine': 3, 'learning': 4, 'coding': 5, 'in': 6, 'python': 7, 'can': 8, 'be': 9, 'fun': 10}\n","\n","토큰화 결과:\n"," [[1, 2, 3, 4], [1, 2, 5, 6, 7], [3, 4, 8, 9, 10]]\n","\n","패딩 결과:\n"," [[ 0  1  2  3  4]\n"," [ 1  2  5  6  7]\n"," [ 3  4  8  9 10]]\n"]}]},{"cell_type":"code","source":["# 주승님 솔루션\n","texts = ['I love machine learning', 'I love coding in Python', 'Machine learning can be fun']\n","\n","token = Tokenizer()\n","token.fit_on_texts(texts)\n","\n","sequences = token.texts_to_sequences(texts)\n","\n","print('고유 인덱스:', token.word_index)\n","print(\"정수 인덱스:\", sequences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fvc5vNXIL5Ua","executionInfo":{"status":"ok","timestamp":1707185681215,"user_tz":-540,"elapsed":2,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"ce99b97e-4922-44d0-97b7-f13c7276c9c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["고유 인덱스: {'i': 1, 'love': 2, 'machine': 3, 'learning': 4, 'coding': 5, 'in': 6, 'python': 7, 'can': 8, 'be': 9, 'fun': 10}\n","정수 인덱스: [[1, 2, 3, 4], [1, 2, 5, 6, 7], [3, 4, 8, 9, 10]]\n"]}]},{"cell_type":"markdown","source":["Q. 위에서 생성한 정수 시퀀스에 패딩을 추가하여 모든 시퀀스의 길이를 동일하게 만드세요."],"metadata":{"id":"jz9jfq0pFRKR"}},{"cell_type":"code","source":["from keras.preprocessing.sequence import pad_sequences\n","\n","# 패딩 추가\n","padded_sequences = pad_sequences(sequences, padding='post') # 뒤로 가게 하려면 post 쓰면 됨\n","\n","# 패딩된 시퀀스 출력\n","print(\"패딩된 정수 시퀸스:\", padded_sequences)\n","# 자동으로 숫자 맞춰서 됨(가장 큰것 기준으로)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aQ7p96f4FS_5","executionInfo":{"status":"ok","timestamp":1707186723303,"user_tz":-540,"elapsed":339,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"940371c0-de72-4b9d-e782-70fb29b19472"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["패딩된 정수 시퀸스: [[ 1  2  3  4  0]\n"," [ 1  2  5  6  7]\n"," [ 3  4  8  9 10]]\n"]}]},{"cell_type":"code","source":["# A. 강사님 솔루션\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","\n","# texts = [\"I love machine learning.\", \"I love coding in Python.\", \"Machine learning can be fun.\"] # 4 / 5 / 5\n","texts = [\"I love coding in Python.\", \"I love machine learning.\", \"Machine learning can be fun.\"] # 5 / 4 / 4\n","# 첫번째 열에 1이 나타나는 이유는 여기서 0 인덱스가 실제로 원-핫 인코딩에서 하나의 위치를 차지하고 있기 때문\n","# 토큰화 및 시퀀스 패딩 과정에서 0은 일반적으로 패딩을 위해 사용.\n","\n","# 토큰화 객체 생성\n","tokenizer = Tokenizer(num_words=1000)\n","\n","# 데이터셋에 대해 토큰화 수행\n","tokenizer.fit_on_texts(texts)\n","\n","# 각 문장을 정수 시퀸스로 변환\n","sequences = tokenizer.texts_to_sequences(texts)\n","print(\"각 문장을 정수 시퀸스로 변환:\\n\", sequences)\n","\n","# 모든 시퀀스를 동일한 길이로 패딩\n","padded_sequences = pad_sequences(sequences, padding='post')\n","print(\"\\n모든 시퀀스를 동일한 길이로 패딩:\\n\", padded_sequences)\n","\n","word_size = len(tokenizer.word_index) + 1\n","x = to_categorical(padded_sequences, num_classes = word_size) # to_categorical 사용\n","\n","print(\"\\n패딩된 시퀀스:\\n\", x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GCKhCTVXmIgz","executionInfo":{"status":"ok","timestamp":1707197059628,"user_tz":-540,"elapsed":442,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"0b0ec1bd-33c0-4819-9117-930a3b2dd6bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["각 문장을 정수 시퀸스로 변환:\n"," [[1, 2, 5, 6, 7], [1, 2, 3, 4], [3, 4, 8, 9, 10]]\n","\n","모든 시퀀스를 동일한 길이로 패딩:\n"," [[ 1  2  5  6  7]\n"," [ 1  2  3  4  0]\n"," [ 3  4  8  9 10]]\n","\n","패딩된 시퀀스:\n"," [[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n","\n"," [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n","  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n"]}]},{"cell_type":"code","source":["# 용규님 솔루션\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","\n","texts = [\"I love machine learning.\", \"I love coding in Python.\", \"Machine learning can be fun.\"]\n","\n","tokenizer = Tokenizer(num_words=1000) # 사용할 최대 단어 수 설정\n","\n","tokenizer.fit_on_texts(texts)\n","\n","sequences = tokenizer.texts_to_sequences(texts)\n","\n","sequences = pad_sequences(sequences, padding='post')\n","\n","word_size = len(tokenizer.word_index) + 1\n","x = to_categorical(sequences, num_classes=word_size)\n","\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MV7v2py9NNMb","executionInfo":{"status":"ok","timestamp":1707186236868,"user_tz":-540,"elapsed":2,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"0c56059a-04f3-46b9-9143-2c45f15a48f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n","  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n","\n"," [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n"]}]},{"cell_type":"markdown","source":["Q. 주어진 텍스트 데이터셋에 대해 원-핫 인코딩을 수행하세요.\n","- 사용할 최대 단어 수 설정 : num_words=1000"],"metadata":{"id":"F7DjVxIoFN50"}},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","\n","# 데이터셋 정의\n","texts = [\"I love machine learning.\", \"I love coding in Python.\", \"Machine learning can be fun.\"]"],"metadata":{"id":"n0jG-3apFOOe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","\n","# 데이터셋 정의\n","texts = [\"I love machine learning.\", \"I love coding in Python.\", \"Machine learning can be fun.\"]\n","\n","tokenizer = Tokenizer(num_words=1000) # 사용할 최대 단어 수 설정\n","\n","tokenizer.fit_on_texts(texts)\n","\n","seq = tokenizer.texts_to_matrix(texts, mode='binary') # 모드가 바이너리인 이유는 원핫인코딩에 결과가 0 과 1 만 나오기 때문.\n","print(seq)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BwMmbUJMPHPx","executionInfo":{"status":"ok","timestamp":1707186434756,"user_tz":-540,"elapsed":3,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"6551efa7-9433-4199-c9c3-50573ce0e043"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 1. 1. ... 0. 0. 0.]\n"," [0. 1. 1. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n"]}]},{"cell_type":"markdown","source":["texts_to_matrix() 함수는 텍스트 데이터를 입력으로 받아 각 문장을 토큰화하고, 지정된 모드(binary, count, tfidf, freq)에 따라 변환된 매트릭스를 반환합니다. binary 모드를 사용하면, 각 단어가 문장 내에 존재하는지 여부에 따라 0 또는 1의 값을 갖는 원-핫 인코딩 매트릭스를 얻을 수 있습니다. 이 경우 to_categorical() 함수는 필요하지 않습니다.\n","\n","따라서 주어진 문제에서는 to_categorical() 함수를 사용하지 않고도 원-핫 인코딩을 수행할 수 있습니다. Tokenizer와 그의 texts_to_matrix() 메소드만으로 충분히 텍스트 데이터를 원-핫 인코딩된 형태로 변환할 수 있기 때문입니다."],"metadata":{"id":"iUr-MhZ4qytH"}},{"cell_type":"markdown","source":["### Embedding\n","- 기계 학습, 특히 자연어 처리(NLP)의 맥락에서 임베딩은 단어, 구문 또는 기타 유형의 엔터티(단어들이 희소행렬로 되어있는것으로 생각하면됨)를 실수의 밀집된 벡터로 표현하는 기술을 의미. 핵심 아이디어는 이러한 엔터티의 의미론적 의미를 연속적인 벡터 공간으로 인코딩하는 것이다. 여기서 벡터 간의 기하학적 관계는 이들이 나타내는 엔터티 간의 의미론적 관계를 반영하며 이 접근 방식은 의미론적 관계가 캡처되지 않는 원-핫 인코딩과 같은 희소 표현과 대조된다.\n","- 의미:\n","  - 의미적 표현: 임베딩 벡터는 단어나 개체의 의미를 포착하도록 설계된다. 비슷한 의미를 가진 단어는 임베딩 공간에서 서로 가까운 벡터를 갖도록 처리된다.\n","  - 차원성 감소: 임베딩은 고차원 공간(예: 원-핫 인코딩된 벡터)의 단어를 저차원의 연속 벡터 공간으로 매핑하며 이는 표현을 더욱 효율적으로 만들고 계산 복잡성을 줄인다.\n","  - 컨텍스트화: 고급 임베딩 모델(예: Word2Vec, GloVe, BERT)에서는 단어가 나타나는 컨텍스트가 벡터 표현에 영향을 미치므로 모델이 다양한 컨텍스트에서 단어의 다양한 의미를 캡처할 수 있다.\n","- 임베딩 생성 방법:\n","  - 사전 훈련된 임베딩: 일반적인 접근 방식 중 하나는 대규모 텍스트 모음에 대해 사전 훈련된 임베딩을 사용하는 것으로 Word2Vec, GloVe와 같은 모델을 사용하면 훈련 코퍼스에서 풍부한 의미 체계 관계를 학습한 사전 훈련된 모델을 기반으로 단어를 벡터에 매핑할 수 있다.\n","    - Word2Vec: 컨텍스트를 사용하여 대상 단어를 예측하거나(CBOW) 단어를 사용하여 컨텍스트를 예측(Skip-gram)하여 임베딩을 학습한다.\n","      - CBOW 모델은 주어진 컨텍스트(주변 단어들)를 바탕으로 타겟 단어를 예측하는 방식으로 작동. 예를 들어, 문장 \"the cat sits on the\"에서 \"cat\", \"sits\", \"on\", \"the\"를 컨텍스트로 사용하여 \"mat\"이라는 타겟 단어를 예측\n","      - skip-gram 모델은 CBOW와 반대로, 주어진 타겟 단어로부터 컨텍스트(주변 단어들)를 예측하는 방식으로 작동. 예를 들어, \"cat\"이라는 단어가 주어졌을 때, \"the\", \"sits\", \"on\", \"the\"와 같은 주변 단어들을 예측\n","    - GloVe: 단어 동시 발생 통계 행렬을 인수분해하여 임베딩을 학습한다.    \n","  - 자신만의 임베딩 훈련: 신경망을 사용하여 처음부터 자신만의 임베딩을 훈련할 수도 있다.\n","\n","\n","- 고양이하고 개가 있고.. 고양이에 해당하는 호랑이. 개에 해당하는 늑대. 의미론적으로 서로 비슷한데 의미론적으로 비슷한것을 묶어서 벡터화(밀집화)하는것이 임베딩. 유사도같은 지수를 끄집어 내서 묶어서 연결해주는 방식으로 만든것이 밀집된 벡터로 표현하는 방법.\n","- 원핫인코딩된 단어들,,, 십만의 단어들을 1000개의 저차원의 연속벡터로 변환시켜줌-효율업. 그때 쓰는것이 Word2Vec, GloVe, BERT등.. 그러므로인해 의미를 쉽게 캡처할 수 있음\n","- 자연어처리 방식에 있어서 Word2Vec이 처음으로 주목 받았던 것\n","- 두가지 방식이 있는데.. CBOW, Skip-gram.. 서로 대조적인 방법\n","- CBOW는 주변 단어를 바탕으로 목표를 하는 타겟단어를 예측하는 방식. 그 고양이가 **위에 앉는다. 예측 단어: 매트\n","- 주어진 단어로 주변 단어들을 예측하는 방식으로 작동. cat 이 있으면 the sits on the 를 예측..\n","- 여튼 비용이 많이든다.."],"metadata":{"id":"nbijy4ulQfr_"}},{"cell_type":"code","source":["/content/drive/MyDrive/hjh_kita_directory/Github/kita_231026/m6_dl/m6_nlp기초.ipynb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KV9qDMaH3i3Z","executionInfo":{"status":"ok","timestamp":1707985528689,"user_tz":-540,"elapsed":437,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"c36c593f-4b68-44f4-cf72-18eba17b9325"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"markdown","source":["텍스트 입력을 표현하기 위해 단어 임베딩 활용\n","- model.add(Embedding(word_size, 8, input_length=4)): 이 줄은 모델에 임베딩 레이어를 추가. 임베딩 레이어는 단어 임베딩을 만드는 데 사용. 각 매개변수의 의미는 다음과 같다.\n","  - word_size: 입력 차원의 크기, 즉 어휘 크기. 데이터 세트에 있는 총 고유 단어 수에 1을 더한 값.\n","  - 8: 임베딩 벡터의 크기. 각 단어는 8차원 벡터로 표현.\n","  - input_length=4: 입력 시퀀스의 길이. 이 모델은 각 입력 시퀀스의 길이가 4일 것으로 예상(예: 입력당 4개의 단어).\n","- model.add(Flatten()): 임베딩 레이어 이후 출력 모양은 배치 크기를 포함하여 3차원. 이 레이어는 출력을 2차원(배치 크기, input_length * 8)으로 평면화하여 밀도가 높은 레이어에 직접 공급될 수 있도록 한다."],"metadata":{"id":"2dSX-YrSUWoA"}},{"cell_type":"code","source":["# 데이터 준비\n","# 텍스트 리뷰 자료를 지정합니다.\n","docs = [\"너무 재밌네요\",\"최고예요\",\"참 잘 만든 영화예요\",\"추천하고 싶은 영화입니다\",\"한번 더 보고싶네요\", \\\n","        \"글쎄요\",\"별로예요\",\"생각보다 지루하네요\",\"연기가 어색해요\",\"재미없어요\"]\n","\n","# 긍정 리뷰는 1, 부정 리뷰는 0으로 클래스를 지정합니다.\n","classes = array([1,1,1,1,1,0,0,0,0,0]) # classes 에는 긍정, 부정 감정을 담음(이진분류). 레이블을 다 달아준것임. 이것이 y임."],"metadata":{"id":"taButij3qW8q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 텍스트 전처리\n","from keras.preprocessing.text import Tokenizer\n","token = Tokenizer()\n","token.fit_on_texts(docs)\n","print(token.word_index)\n","\n","# 임베딩에 입력될 단어의 수를 지정합니다.\n","word_size = len(token.word_index) +1\n","\n","# 모델 구성\n","# 단어 임베딩을 포함하여 딥러닝 모델을 만들고 결과를 출력합니다.\n","model = Sequential()\n","model.add(Embedding(word_size, 8, input_length=4)) # (None, 4, 8) 4는 시퀀스 길이. 8은 임베딩 차원.\n","# 8: 임베딩 차원(embedding dimension)입니다. Embedding 레이어는 각 단어(또는 토큰)를 고정된 크기의 벡터로 변환하는 역할을 합니다.\n","# 여기서는 각 단어가 8차원의 벡터로 변환된다는 것을 의미합니다. 따라서, 각 단어는 8개의 실수로 이루어진 벡터로 표현\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EN3pOEAWUYuu","executionInfo":{"status":"ok","timestamp":1707193759445,"user_tz":-540,"elapsed":375,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"1ca45f09-7d21-4ed3-aeba-9f9cf0fd228f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n","Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_5 (Embedding)     (None, 4, 8)              168       \n","                                                                 \n"," flatten_5 (Flatten)         (None, 32)                0         \n","                                                                 \n"," dense_5 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 201 (804.00 Byte)\n","Trainable params: 201 (804.00 Byte)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# 입력 데이터 준비\n","x = token.texts_to_sequences(docs)\n","padded_x = pad_sequences(x, 4, padding='pre')"],"metadata":{"id":"WhE7f3m_p0jx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 컴파일 및 훈련\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","model.fit(padded_x, classes, epochs=20)\n","\n","# 평가\n","print(\"\\nAccuracy: %.4f\" % (model.evaluate(padded_x, classes)[1])) # padded_x 가 X고 classes가 y 라고 생각하면 됨. data, target(class)\n","# 여기서 [1] 한 이유. loss 하고 accuracy 두개가 나오는데 뒤에것인 accuracy를 뽑아준 것임."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6w8wEerIrztg","executionInfo":{"status":"ok","timestamp":1707193997134,"user_tz":-540,"elapsed":2298,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"90c0608f-cd85-4225-e802-797d9d1b4394"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","1/1 [==============================] - 1s 969ms/step - loss: 0.7040 - accuracy: 0.3000\n","Epoch 2/20\n","1/1 [==============================] - 0s 11ms/step - loss: 0.7021 - accuracy: 0.3000\n","Epoch 3/20\n","1/1 [==============================] - 0s 12ms/step - loss: 0.7002 - accuracy: 0.4000\n","Epoch 4/20\n","1/1 [==============================] - 0s 11ms/step - loss: 0.6984 - accuracy: 0.4000\n","Epoch 5/20\n","1/1 [==============================] - 0s 12ms/step - loss: 0.6965 - accuracy: 0.5000\n","Epoch 6/20\n","1/1 [==============================] - 0s 11ms/step - loss: 0.6947 - accuracy: 0.5000\n","Epoch 7/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.5000\n","Epoch 8/20\n","1/1 [==============================] - 0s 14ms/step - loss: 0.6910 - accuracy: 0.6000\n","Epoch 9/20\n","1/1 [==============================] - 0s 12ms/step - loss: 0.6892 - accuracy: 0.7000\n","Epoch 10/20\n","1/1 [==============================] - 0s 14ms/step - loss: 0.6873 - accuracy: 0.7000\n","Epoch 11/20\n","1/1 [==============================] - 0s 15ms/step - loss: 0.6855 - accuracy: 0.8000\n","Epoch 12/20\n","1/1 [==============================] - 0s 11ms/step - loss: 0.6837 - accuracy: 0.9000\n","Epoch 13/20\n","1/1 [==============================] - 0s 13ms/step - loss: 0.6818 - accuracy: 0.9000\n","Epoch 14/20\n","1/1 [==============================] - 0s 14ms/step - loss: 0.6800 - accuracy: 0.9000\n","Epoch 15/20\n","1/1 [==============================] - 0s 11ms/step - loss: 0.6782 - accuracy: 1.0000\n","Epoch 16/20\n","1/1 [==============================] - 0s 12ms/step - loss: 0.6763 - accuracy: 1.0000\n","Epoch 17/20\n","1/1 [==============================] - 0s 11ms/step - loss: 0.6745 - accuracy: 1.0000\n","Epoch 18/20\n","1/1 [==============================] - 0s 12ms/step - loss: 0.6726 - accuracy: 1.0000\n","Epoch 19/20\n","1/1 [==============================] - 0s 13ms/step - loss: 0.6707 - accuracy: 1.0000\n","Epoch 20/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6688 - accuracy: 1.0000\n","1/1 [==============================] - 0s 167ms/step - loss: 0.6669 - accuracy: 1.0000\n","\n","Accuracy: 1.0000\n"]}]},{"cell_type":"markdown","source":["Q. 10개의 단어를 가진 어휘(vocabulary)와 각 단어를 4차원 벡터로 임베딩하는 Keras 모델을 구성하세요. 시퀀스의 최대 길이는 5로 설정하세요."],"metadata":{"id":"bgXpUUR6uiI7"}},{"cell_type":"code","source":["# A.\n","vocabulary = [\n","    \"너무 재밌네요\",\"최고예요\",\"참 잘 만든 영화예요\",\"추천하고 싶은 영화입니다\",\"한번 더 보고싶네요\", \\\n","    \"글쎄요\",\"별로예요\",\"생각보다 지루하네요\",\"연기가 어색해요\",\"재미없어요\"\n","]\n","\n","classes = array([1,1,1,1,1,0,0,0,0,0])\n","\n","token = Tokenizer()\n","token.fit_on_texts(vocabulary)\n","print(token.word_index)\n","\n","word_size = len(token.word_index) + 1\n","\n","model = Sequential()\n","model.add(Embedding(word_size, 4, input_length=5))\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i42CytYPuvwd","executionInfo":{"status":"ok","timestamp":1707195145274,"user_tz":-540,"elapsed":435,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"bb735ca6-8f74-42c1-8c4d-c1ce09fc704c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n","Model: \"sequential_9\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_9 (Embedding)     (None, 5, 4)              84        \n","                                                                 \n"," flatten_9 (Flatten)         (None, 20)                0         \n","                                                                 \n"," dense_8 (Dense)             (None, 1)                 21        \n","                                                                 \n","=================================================================\n","Total params: 105 (420.00 Byte)\n","Trainable params: 105 (420.00 Byte)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# A. 강사님 솔루션. 임베딩까지만 모델 구성.\n","from keras.models import Sequential\n","from keras.layers import Embedding\n","\n","model = Sequential()\n","model.add(Embedding(input_dim=10, output_dim=4, input_length=5)) # 인풋디멘션-10차원 인것을 아웃풋디멘션-4차원으로 임베딩. 시퀀스의 최대길이5\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"onBslkz70iXP","executionInfo":{"status":"ok","timestamp":1707196330863,"user_tz":-540,"elapsed":327,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"9efe4790-e208-4144-bdde-cd62a1d797e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_10\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_10 (Embedding)    (None, 5, 4)              40        \n","                                                                 \n","=================================================================\n","Total params: 40 (160.00 Byte)\n","Trainable params: 40 (160.00 Byte)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["Q. 100개의 단어를 가진 어휘와 각 단어를 8차원 벡터로 임베딩한 후, 이를 평탄화하고 Dense 레이어를 통해 분류하는 Keras 모델을 구성하세요. 최종 출력은 3개의 클래스를 가진 분류 문제를 가정"],"metadata":{"id":"RGKyr6cEui7X"}},{"cell_type":"code","source":["# A. 모델 구성 add 까지\n","from keras.models import Sequential\n","from keras.layers import Embedding\n","\n","# 모델 구성\n","model = Sequential()\n","model.add(Embedding(input_dim=100, output_dim=8, input_length=10))\n","model.add(Flatten())\n","model.add(Dense(3, activation='softmax')) # 멀티. 3개. 그래서 softmax 사용\n","\n","# 모델 요약 출력\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mt8DXak41hQZ","executionInfo":{"status":"ok","timestamp":1707196561315,"user_tz":-540,"elapsed":343,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"ad151ed7-2173-4c24-d608-19d1ba270999"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_11\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_11 (Embedding)    (None, 10, 8)             800       \n","                                                                 \n"," flatten_10 (Flatten)        (None, 80)                0         \n","                                                                 \n"," dense_9 (Dense)             (None, 3)                 243       \n","                                                                 \n","=================================================================\n","Total params: 1043 (4.07 KB)\n","Trainable params: 1043 (4.07 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["Q. 간단한 분류 모델을 구성하고, 이를 컴파일하는 코드를 작성하세요. 이 모델은 이진 분류 문제를 해결하며, 다음 조건을 만족해야 한다:\n","\n","- 입력 차원은 20.\n","- 첫 번째 Dense 레이어는 64 유닛과 ReLU 활성화 함수를 사용.\n","- 출력 레이어는 1 유닛과 시그모이드 활성화 함수를 사용.\n","- 손실 함수로는 binary crossentropy를 사용.\n","- 옵티마이저로는 'adam'을 사용.\n","- 평가 지표로는 'accuracy'를 사용."],"metadata":{"id":"5VrPZ42Lujzo"}},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Dense\n","\n","# 모델 구성\n","model = Sequential()\n","model.add(Dense(64, activation='relu', input_dim=20))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# 모델 컴파일\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","# 모델 요약 출력\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yzmY0i5M2yGN","executionInfo":{"status":"ok","timestamp":1707196852301,"user_tz":-540,"elapsed":349,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"f81cee4f-890e-4aa7-86c8-47772ee7a87f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_12\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_10 (Dense)            (None, 64)                1344      \n","                                                                 \n"," dense_11 (Dense)            (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 1409 (5.50 KB)\n","Trainable params: 1409 (5.50 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["Q1_0206. IMDB 영화 리뷰 데이터셋을 사용하여 긍부정 이진분류 모델링 및 평가를 수행하세요. 단, embedding 차원은 8"],"metadata":{"id":"iWmBA-bg5hPr"}},{"cell_type":"code","source":["from keras.datasets import imdb\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Embedding, Flatten, Dense\n","\n","# 데이터셋 로드\n","# num_words=10000은 훈련 데이터에서 가장 자주 나타나는 상위 10,000개의 단어만 사용하겠다는 의미입니다.\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n","\n","# 시퀀스 데이터 패딩\n","x_train = pad_sequences(x_train, maxlen=100)\n","x_test = pad_sequences(x_test, maxlen=100)"],"metadata":{"id":"9lhC8MQM5okF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Sequential()\n","model.add(Embedding(input_dim=10000, output_dim=8, input_length=100))\n","# num_words=10000으로 설정했으므로 input_dim도 10000이 되어야 함.\n","# pad_sequences를 사용하여 길이를 100으로 설정했으므로, input_length도 100이 되어야 함.\n","model.add(Flatten())\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-DdlGcul6WNV","executionInfo":{"status":"ok","timestamp":1707200806899,"user_tz":-540,"elapsed":438,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"3f4b5b62-c82b-4932-b26d-4e2b2bb7d90b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_16\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_15 (Embedding)    (None, 100, 8)            80000     \n","                                                                 \n"," flatten_14 (Flatten)        (None, 800)               0         \n","                                                                 \n"," dense_14 (Dense)            (None, 32)                25632     \n","                                                                 \n"," dense_15 (Dense)            (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 105665 (412.75 KB)\n","Trainable params: 105665 (412.75 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.fit(x_train, y_train, epochs=20) # batch_size는 기본값으로 설정됩니다(일반적으로 32)\n","\n","# print(model.evaluate(x_test, y_test))\n","# 모델을 평가하려면 model.evaluate(x_test, y_test)를 사용해야 하며, 예측을 수행하려면 model.predict(x_test)를 사용해야 함.\n","\n","loss, accuracy = model.evaluate(x_test, y_test) # 이렇게 해도 됨.\n","print(f\"Accuracy:, {accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nuj45jmL-C3q","executionInfo":{"status":"ok","timestamp":1707200856399,"user_tz":-540,"elapsed":44583,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"a151a87d-6d50-4156-f087-2193976e7221"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","782/782 [==============================] - 3s 2ms/step - loss: 0.4581 - accuracy: 0.7674\n","Epoch 2/20\n","782/782 [==============================] - 3s 4ms/step - loss: 0.2240 - accuracy: 0.9126\n","Epoch 3/20\n","782/782 [==============================] - 2s 2ms/step - loss: 0.0962 - accuracy: 0.9700\n","Epoch 4/20\n","782/782 [==============================] - 2s 2ms/step - loss: 0.0272 - accuracy: 0.9948\n","Epoch 5/20\n","782/782 [==============================] - 2s 2ms/step - loss: 0.0060 - accuracy: 0.9995\n","Epoch 6/20\n","782/782 [==============================] - 2s 2ms/step - loss: 0.0016 - accuracy: 0.9999\n","Epoch 7/20\n","782/782 [==============================] - 2s 2ms/step - loss: 6.3172e-04 - accuracy: 1.0000\n","Epoch 8/20\n","782/782 [==============================] - 3s 4ms/step - loss: 3.3218e-04 - accuracy: 1.0000\n","Epoch 9/20\n","782/782 [==============================] - 2s 2ms/step - loss: 1.9486e-04 - accuracy: 1.0000\n","Epoch 10/20\n","782/782 [==============================] - 2s 2ms/step - loss: 1.2016e-04 - accuracy: 1.0000\n","Epoch 11/20\n","782/782 [==============================] - 2s 2ms/step - loss: 7.3552e-05 - accuracy: 1.0000\n","Epoch 12/20\n","782/782 [==============================] - 2s 2ms/step - loss: 4.5538e-05 - accuracy: 1.0000\n","Epoch 13/20\n","782/782 [==============================] - 2s 2ms/step - loss: 2.8717e-05 - accuracy: 1.0000\n","Epoch 14/20\n","782/782 [==============================] - 3s 4ms/step - loss: 1.8238e-05 - accuracy: 1.0000\n","Epoch 15/20\n","782/782 [==============================] - 2s 2ms/step - loss: 1.1500e-05 - accuracy: 1.0000\n","Epoch 16/20\n","782/782 [==============================] - 2s 2ms/step - loss: 7.3684e-06 - accuracy: 1.0000\n","Epoch 17/20\n","782/782 [==============================] - 2s 2ms/step - loss: 4.6915e-06 - accuracy: 1.0000\n","Epoch 18/20\n","782/782 [==============================] - 2s 2ms/step - loss: 3.0234e-06 - accuracy: 1.0000\n","Epoch 19/20\n","782/782 [==============================] - 2s 2ms/step - loss: 1.9421e-06 - accuracy: 1.0000\n","Epoch 20/20\n","782/782 [==============================] - 3s 4ms/step - loss: 1.2539e-06 - accuracy: 1.0000\n","782/782 [==============================] - 1s 1ms/step - loss: 1.3879 - accuracy: 0.8289\n","Accuracy:, 0.8289\n"]}]},{"cell_type":"code","source":["# 주승님 솔루션\n","# Q7_0206. IMDB 영화 리뷰 데이터셋을 사용하여 긍/부정 이진분류 모델링 및 평가를 수행하세요. Embedding 차원 8\n","from keras.datasets import imdb\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Embedding, Flatten, Dense\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","# 데이터셋 로드\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = 10000)\n","\n","# 시퀀스 데이터 패딩\n","x_train = pad_sequences(x_train, maxlen = 100)\n","x_test = pad_sequences(x_test, maxlen = 100)\n","\n","# 모델 구축\n","word_size = 10000\n","model = Sequential([\n","    Embedding(word_size, 8, input_length = 100),\n","    Flatten(),\n","    Dense(10, activation = 'relu'),\n","    Dense(1, activation='sigmoid')  # 이진분류는 출력 뉴런 수를 1로 설정\n","])\n","\n","# 모델 요약 출력\n","model.summary()\n","\n","# EarlyStopping 설정\n","Early_Stopping_Callbacks = EarlyStopping(monitor = 'val_accuracy', patience = 20)\n","\n","# 모델 컴파일 및 결과 출력\n","model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","model.fit(x_train, y_train, epochs = 20000, batch_size = 20, verbose = 1, \\\n","          validation_split = 0.25, callbacks = [Early_Stopping_Callbacks])\n","print('\\nAccuracy: %.4f' % model.evaluate(x_test, y_test)[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GA5ReABKGihL","executionInfo":{"status":"ok","timestamp":1707200984309,"user_tz":-540,"elapsed":71449,"user":{"displayName":"한정현","userId":"04742589720279403748"}},"outputId":"09a8e962-abdd-4c17-db39-c752a88142ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_17\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_16 (Embedding)    (None, 100, 8)            80000     \n","                                                                 \n"," flatten_15 (Flatten)        (None, 800)               0         \n","                                                                 \n"," dense_16 (Dense)            (None, 10)                8010      \n","                                                                 \n"," dense_17 (Dense)            (None, 1)                 11        \n","                                                                 \n","=================================================================\n","Total params: 88021 (343.83 KB)\n","Trainable params: 88021 (343.83 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 0.4927 - accuracy: 0.7454 - val_loss: 0.3433 - val_accuracy: 0.8469\n","Epoch 2/20000\n","938/938 [==============================] - 4s 4ms/step - loss: 0.2329 - accuracy: 0.9096 - val_loss: 0.3500 - val_accuracy: 0.8490\n","Epoch 3/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 0.1094 - accuracy: 0.9675 - val_loss: 0.4302 - val_accuracy: 0.8362\n","Epoch 4/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 0.0377 - accuracy: 0.9925 - val_loss: 0.5258 - val_accuracy: 0.8328\n","Epoch 5/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 0.0123 - accuracy: 0.9983 - val_loss: 0.6158 - val_accuracy: 0.8285\n","Epoch 6/20000\n","938/938 [==============================] - 3s 4ms/step - loss: 0.0035 - accuracy: 0.9997 - val_loss: 0.6797 - val_accuracy: 0.8323\n","Epoch 7/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 9.6215e-04 - accuracy: 1.0000 - val_loss: 0.7339 - val_accuracy: 0.8317\n","Epoch 8/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 4.5137e-04 - accuracy: 1.0000 - val_loss: 0.7819 - val_accuracy: 0.8318\n","Epoch 9/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 2.4142e-04 - accuracy: 1.0000 - val_loss: 0.8321 - val_accuracy: 0.8318\n","Epoch 10/20000\n","938/938 [==============================] - 3s 4ms/step - loss: 1.3778e-04 - accuracy: 1.0000 - val_loss: 0.8785 - val_accuracy: 0.8314\n","Epoch 11/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 7.6178e-05 - accuracy: 1.0000 - val_loss: 0.9242 - val_accuracy: 0.8306\n","Epoch 12/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 4.3729e-05 - accuracy: 1.0000 - val_loss: 0.9727 - val_accuracy: 0.8302\n","Epoch 13/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 2.5185e-05 - accuracy: 1.0000 - val_loss: 1.0133 - val_accuracy: 0.8302\n","Epoch 14/20000\n","938/938 [==============================] - 4s 4ms/step - loss: 1.4626e-05 - accuracy: 1.0000 - val_loss: 1.0625 - val_accuracy: 0.8304\n","Epoch 15/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 8.5585e-06 - accuracy: 1.0000 - val_loss: 1.1097 - val_accuracy: 0.8306\n","Epoch 16/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 5.0527e-06 - accuracy: 1.0000 - val_loss: 1.1525 - val_accuracy: 0.8296\n","Epoch 17/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 2.9372e-06 - accuracy: 1.0000 - val_loss: 1.1956 - val_accuracy: 0.8304\n","Epoch 18/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 1.7361e-06 - accuracy: 1.0000 - val_loss: 1.2404 - val_accuracy: 0.8288\n","Epoch 19/20000\n","938/938 [==============================] - 3s 4ms/step - loss: 1.0455e-06 - accuracy: 1.0000 - val_loss: 1.2856 - val_accuracy: 0.8296\n","Epoch 20/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 6.1608e-07 - accuracy: 1.0000 - val_loss: 1.3303 - val_accuracy: 0.8293\n","Epoch 21/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 3.7056e-07 - accuracy: 1.0000 - val_loss: 1.3725 - val_accuracy: 0.8291\n","Epoch 22/20000\n","938/938 [==============================] - 2s 3ms/step - loss: 2.2790e-07 - accuracy: 1.0000 - val_loss: 1.4170 - val_accuracy: 0.8285\n","782/782 [==============================] - 1s 2ms/step - loss: 1.4492 - accuracy: 0.8280\n","\n","Accuracy: 0.8280\n"]}]}]}